{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWnNIjCE-8Dx"
   },
   "source": [
    "## Assignment : 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZPYDuDH-8D3"
   },
   "source": [
    "<pre>\n",
    "1. You can work with preprocessed_data.csv for the assignment. You can get the data from - <a href='https://drive.google.com/drive/u/0/folders/1CJnItndeSSJu7aragQoXWZS9-0apN6pp'>Data folder </a>\n",
    "2. Load the data in your notebook.\n",
    "3. After step 2 you have to train 3 types of models as discussed below. \n",
    "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a  href='https://stackoverflow.com/a/46844409'>this</a> and <a  href='https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807'>this</a> for using auc as a metric \n",
    "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
    "6. You can use any one of the optimizers and choice of Learning rate and momentum.\n",
    "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in a separate pad and write your observations about them.\n",
    "8. Make sure that you are using GPU to train the given models.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLeprAk9-8D5"
   },
   "outputs": [],
   "source": [
    "#you can use gdown modules to import dataset for the assignment\n",
    "#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n",
    "#you can run the below cell to import the required preprocessed data.csv file and glove vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6tEhqlh-8D7"
   },
   "outputs": [],
   "source": [
    "#!gdown --id 1GpATd_pM4mcnWWIs28-s1lgqdAg2Wdv-\n",
    "#!gdown --id 1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIZMmpPi_EKm",
    "outputId": "f1244086-44b0-4397-f753-ea3e96f2fc41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHO2c7lB-8D8"
   },
   "source": [
    "## <font color='red'> Model-1 </font>\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5nrqggA-8D8"
   },
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXXY3ip3-8D9"
   },
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbKS5YkM-8D_"
   },
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAHfulOg-8EA"
   },
   "outputs": [],
   "source": [
    "'''# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "input_layer = Input(shape=(n,))\n",
    "embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n",
    "flatten = Flatten()(embedding)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3-rZzcE-8EB"
   },
   "source": [
    "### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yISGc08q-8EC"
   },
   "source": [
    "# <font color='red'> Model-1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u9e8mbuI-8EC"
   },
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "#make sure that you import your libraries from tf.keras and not just keras\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "BXb0xDUN-8ED",
    "outputId": "84e0bd71-40de-4c24-fbc2-15cfe5857c1c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-bc907527-372c-4076-b395-faf9c26be393\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>having class 24 students comes diverse learner...</td>\n",
       "      <td>329.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>appliedlearning</td>\n",
       "      <td>earlydevelopment</td>\n",
       "      <td>i recently read article giving students choice...</td>\n",
       "      <td>481.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>my students crave challenge eat obstacles brea...</td>\n",
       "      <td>17.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc907527-372c-4076-b395-faf9c26be393')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-bc907527-372c-4076-b395-faf9c26be393 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-bc907527-372c-4076-b395-faf9c26be393');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "2           ca            mrs          grades_prek_2   \n",
       "3           ga            mrs          grades_prek_2   \n",
       "4           wa            mrs             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "2                                            10                    1   \n",
       "3                                             2                    1   \n",
       "4                                             2                    1   \n",
       "\n",
       "    clean_categories                 clean_subcategories  \\\n",
       "0       math_science  appliedsciences health_lifescience   \n",
       "1       specialneeds                        specialneeds   \n",
       "2  literacy_language                            literacy   \n",
       "3    appliedlearning                    earlydevelopment   \n",
       "4  literacy_language                            literacy   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  \n",
       "2  having class 24 students comes diverse learner...  329.00  \n",
       "3  i recently read article giving students choice...  481.04  \n",
       "4  my students crave challenge eat obstacles brea...   17.74  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the csv file\n",
    "df = pd.read_csv('/content/drive/MyDrive/Copy of preprocessed_data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AXCqL3c1_WvE"
   },
   "outputs": [],
   "source": [
    "x=df.drop('project_is_approved',axis=1)\n",
    "y=df['project_is_approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E7DvHeWl-8ED"
   },
   "outputs": [],
   "source": [
    "# perform stratified train test split on the dataset\n",
    "x_train_1, x_test, y_train_1, y_test=train_test_split(x,y,test_size=0.2, stratify=y)\n",
    "x_train, x_cv, y_train, y_cv=train_test_split(x_train_1,y_train_1,test_size=0.2, stratify=y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lpdkOYk7kqjh"
   },
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) \n",
    "y_cv = tf.keras.utils.to_categorical(y_cv, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHDfeW9DrsRJ",
    "outputId": "1316b0f6-afc5-4b7a-b820-84b39b1d7d4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "project_is_approved\n",
       "0    16542\n",
       "1    92706\n",
       "Name: school_state, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_imbalance=df.groupby('project_is_approved').count()['school_state']\n",
    "class_imbalance\n",
    "#The data is heavily imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HeDplMaK6S3B"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "class_weight = compute_class_weight(\"balanced\", classes= np.unique(y),y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8D-cc9e6fcp",
    "outputId": "2a8ffe54-975b-445b-facf-b888c0c14eff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.30214001, 0.58921753])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XNY3-e-8EE"
   },
   "source": [
    "## 1.1 Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5fwDUn80-8EE"
   },
   "outputs": [],
   "source": [
    "#since the data is already preprocessed, we can directly move to vectorization part\n",
    "#first we will vectorize the text data\n",
    "#for vectorization of text data in deep learning we use tokenizer, you can go through below references\n",
    "# https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n",
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "# after text vectorization you should get train_padded_docs and test_padded_docs\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='UNK')\n",
    "tokenizer.fit_on_texts(x_train['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cpaFFV4X-8EF"
   },
   "outputs": [],
   "source": [
    "essay_train=tokenizer.texts_to_sequences(x_train['essay'])\n",
    "essay_test=tokenizer.texts_to_sequences(x_test['essay'])\n",
    "essay_cv=tokenizer.texts_to_sequences(x_cv['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rsuOTcXYEySV"
   },
   "outputs": [],
   "source": [
    "max_words=max([len(i) for i in essay_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ojB3rmNxFsKy"
   },
   "outputs": [],
   "source": [
    "padded_essay_train=tf.keras.preprocessing.sequence.pad_sequences(essay_train,maxlen=max_words,dtype='int32',padding='post',truncating='post')\n",
    "padded_essay_test=tf.keras.preprocessing.sequence.pad_sequences(essay_test,maxlen=max_words,dtype='int32',padding='post',truncating='post')\n",
    "padded_essay_cv=tf.keras.preprocessing.sequence.pad_sequences(essay_cv,maxlen=max_words,dtype='int32',padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3f8Qb_e-8EF",
    "outputId": "4af878ae-0ea0-4f3d-d8c8-11679ddcd2b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:20, 19390.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#after getting the padded_docs you have to use predefined glove vectors to get 300 dim representation for each word\n",
    "# we will be storing this data in form of an embedding matrix and will use it while defining our model\n",
    "# Please go through following blog's 'Example of Using Pre-Trained GloVe Embedding' section to understand how to create embedding matrix\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "glove='/content/drive/MyDrive/Copy of glove.6B.100d.txt'\n",
    "from tqdm import tqdm\n",
    "embeddings_index = dict()\n",
    "f = open(glove)\n",
    "for line in tqdm(f):\n",
    " values = line.split()\n",
    " word = values[0]\n",
    " coefs = np.array(values[1:], dtype='float32')\n",
    " embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekemsz_s-8EF",
    "outputId": "c46cd554-42e6-47c5-b753-4d69abd2e875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47276"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzAS4VOpZSAJ",
    "outputId": "3b371157-afa7-418d-9da4-bdcbef4a3c0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47275/47275 [00:00<00:00, 440550.74it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tqdm(tokenizer.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEkEI-xpXQuV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbAzKd20-8EG"
   },
   "source": [
    "## 1.2 Categorical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEyDsDNM-8EG"
   },
   "outputs": [],
   "source": [
    "# for model 1 and model 2, we have to assign a unique number to each feature in a particular categorical column.\n",
    "# you can either use tokenizer,label encoder or ordinal encoder to perform the task\n",
    "# label encoder gives an error for 'unseen values' (values present in test but not in train)\n",
    "# handle unseen values with label encoder - https://stackoverflow.com/a/56876351\n",
    "# ordinal encoder also gives error with unseen values but you can use modify handle_unknown parameter\n",
    "# documentation of ordianl encoder https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
    "# after categorical feature vectorization you will have column_train_data and column_test_data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "fw9-DnBTcmg1",
    "outputId": "2a2d8d86-7dbd-44e3-a53e-8de123440266"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-520382fe-b826-44b2-8d53-668a755460ea\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-520382fe-b826-44b2-8d53-668a755460ea')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-520382fe-b826-44b2-8d53-668a755460ea button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-520382fe-b826-44b2-8d53-668a755460ea');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects clean_categories  \\\n",
       "0                                            53     math_science   \n",
       "1                                             4     specialneeds   \n",
       "\n",
       "                  clean_subcategories  \\\n",
       "0  appliedsciences health_lifescience   \n",
       "1                        specialneeds   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yTqvDj-m-8EG"
   },
   "outputs": [],
   "source": [
    "state_tokenizer=tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='UNK')\n",
    "state_tokenizer.fit_on_texts(x_train['school_state'])\n",
    "\n",
    "state_train=state_tokenizer.texts_to_sequences(x_train['school_state'])\n",
    "state_test=state_tokenizer.texts_to_sequences(x_test['school_state'])\n",
    "state_cv=state_tokenizer.texts_to_sequences(x_cv['school_state'])\n",
    "\n",
    "state_max_length=max([len(i) for i in state_train])\n",
    "\n",
    "padded_state_train=tf.keras.preprocessing.sequence.pad_sequences(state_train,maxlen=state_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_state_test=tf.keras.preprocessing.sequence.pad_sequences(state_test,maxlen=state_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_state_cv=tf.keras.preprocessing.sequence.pad_sequences(state_cv,maxlen=state_max_length,dtype='int32',padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lICJO8MdN2T",
    "outputId": "4c722caa-b9c9-4c30-ce32-ffeb9c2db546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "prefix_tokenizer=tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='UNK')\n",
    "prefix_tokenizer.fit_on_texts(x_train['teacher_prefix'])\n",
    "\n",
    "prefix_train=prefix_tokenizer.texts_to_sequences(x_train['teacher_prefix'])\n",
    "prefix_test=prefix_tokenizer.texts_to_sequences(x_test['teacher_prefix'])\n",
    "prefix_cv=prefix_tokenizer.texts_to_sequences(x_cv['teacher_prefix'])\n",
    "\n",
    "prefix_max_length=max(list(len(i) for i in prefix_train))\n",
    "print(prefix_max_length)\n",
    "\n",
    "padded_prefix_train=tf.keras.preprocessing.sequence.pad_sequences(prefix_train,maxlen=prefix_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_prefix_test=tf.keras.preprocessing.sequence.pad_sequences(prefix_test,maxlen=prefix_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_prefix_cv=tf.keras.preprocessing.sequence.pad_sequences(prefix_cv,maxlen=prefix_max_length,dtype='int32',padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8o_7NbJYd1xK",
    "outputId": "c8100511-15eb-44b1-a3e5-917e7b3d1fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "grade_tokenizer=tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='UNK')\n",
    "grade_tokenizer.fit_on_texts(x_train['project_grade_category'])\n",
    "\n",
    "grade_train=grade_tokenizer.texts_to_sequences(x_train['project_grade_category'])\n",
    "grade_test=grade_tokenizer.texts_to_sequences(x_test['project_grade_category'])\n",
    "grade_cv=grade_tokenizer.texts_to_sequences(x_cv['project_grade_category'])\n",
    "\n",
    "grade_max_length=max(list(len(i) for i in grade_train))\n",
    "print(grade_max_length)\n",
    "\n",
    "padded_grade_train=tf.keras.preprocessing.sequence.pad_sequences(grade_train,maxlen=grade_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_grade_test=tf.keras.preprocessing.sequence.pad_sequences(grade_test,maxlen=grade_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_grade_cv=tf.keras.preprocessing.sequence.pad_sequences(grade_cv,maxlen=grade_max_length,dtype='int32',padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkZoQ6Ir-8EH",
    "outputId": "25dabd59-c682-4eec-da0c-d8bddaf45d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "cat_tokenizer=tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='UNK')\n",
    "cat_tokenizer.fit_on_texts(x_train['clean_categories'])\n",
    "\n",
    "cat_train=cat_tokenizer.texts_to_sequences(x_train['clean_categories'])\n",
    "cat_test=cat_tokenizer.texts_to_sequences(x_test['clean_categories'])\n",
    "cat_cv=cat_tokenizer.texts_to_sequences(x_cv['clean_categories'])\n",
    "\n",
    "cat_max_length=max(list(len(i) for i in cat_train))\n",
    "print(cat_max_length)\n",
    "\n",
    "padded_cat_train=tf.keras.preprocessing.sequence.pad_sequences(cat_train,maxlen=cat_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_cat_test=tf.keras.preprocessing.sequence.pad_sequences(cat_test,maxlen=cat_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_cat_cv=tf.keras.preprocessing.sequence.pad_sequences(cat_cv,maxlen=cat_max_length,dtype='int32',padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2aIO-bO-8EH",
    "outputId": "8801bdf5-fd57-4683-fad5-3e64acb59f65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "subcat_tokenizer=tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='UNK')\n",
    "subcat_tokenizer.fit_on_texts(x_train['clean_subcategories'])\n",
    "\n",
    "subcat_train=subcat_tokenizer.texts_to_sequences(x_train['clean_subcategories'])\n",
    "subcat_test=subcat_tokenizer.texts_to_sequences(x_test['clean_subcategories'])\n",
    "subcat_cv=subcat_tokenizer.texts_to_sequences(x_cv['clean_subcategories'])\n",
    "\n",
    "subcat_max_length=max(list(len(i) for i in subcat_train))\n",
    "print(subcat_max_length)\n",
    "\n",
    "padded_subcat_train=tf.keras.preprocessing.sequence.pad_sequences(subcat_train,maxlen=subcat_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_subcat_test=tf.keras.preprocessing.sequence.pad_sequences(subcat_test,maxlen=subcat_max_length,dtype='int32',padding='post',truncating='post')\n",
    "padded_subcat_cv=tf.keras.preprocessing.sequence.pad_sequences(subcat_cv,maxlen=subcat_max_length,dtype='int32',padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GKBJvey-8EH"
   },
   "source": [
    "## 1.3 Numerical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYgFSykC-8EH"
   },
   "outputs": [],
   "source": [
    "# you have to standardise the numerical columns\n",
    "# stack both the numerical features\n",
    "#after numerical feature vectorization you will have numerical_data_train and numerical_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KtnuUcLy-8EI"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ge4cSXrV-8EI"
   },
   "outputs": [],
   "source": [
    "no_of_proj_stand=StandardScaler()\n",
    "no_of_proj_stand.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "\n",
    "standardized_no_proj_train=no_of_proj_stand.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "standardized_no_proj_test=no_of_proj_stand.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "standardized_no_proj_cv=no_of_proj_stand.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uHa73p6XSPz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "80fAqVQGhXIZ"
   },
   "outputs": [],
   "source": [
    "x_train['price']\n",
    "price_stand=StandardScaler()\n",
    "price_stand.fit(x_train['price'].values.reshape(-1,1))\n",
    "\n",
    "price_train=price_stand.transform(x_train['price'].values.reshape(-1,1))\n",
    "price_test=price_stand.transform(x_test['price'].values.reshape(-1,1))\n",
    "price_cv=price_stand.transform(x_cv['price'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "G40VQXWWgUbj"
   },
   "outputs": [],
   "source": [
    "numerical_train=np.hstack((standardized_no_proj_train,price_train))\n",
    "numerical_test=np.hstack((standardized_no_proj_test,price_test))\n",
    "numerical_cv=np.hstack((standardized_no_proj_cv,price_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSX6_JOj-8EI"
   },
   "source": [
    "## 1.4 Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO7qH4mF-8EI"
   },
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u85dT3cD-8EJ"
   },
   "outputs": [],
   "source": [
    "# as of now we have vectorized all our features now we will define our model.\n",
    "# as it is clear from above image that the given model has multiple input layers and hence we have to use functional API\n",
    "# Please go through - https://keras.io/guides/functional_api/\n",
    "# it is a good programming practise to define your complete model i.e all inputs , intermediate and output layers at one place.\n",
    "# while defining your model make sure that you use variable names while defining any length,dimension or size.\n",
    "#for ex.- you should write the code as 'input_text = Input(shape=(pad_length,))' and not as 'input_text = Input(shape=(300,))'\n",
    "# the embedding layer for text data should be non trainable\n",
    "# the embedding layer for categorical data should be trainable\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "# https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0\n",
    "#print model.summary() after you have defined the model\n",
    "#plot the model using utils.plot_model module and make sure that it is similar to the above image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZQgJH1Gx16Mu"
   },
   "outputs": [],
   "source": [
    "activation_function='relu'\n",
    "dropout=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnZpi-kP16UU",
    "outputId": "96280186-221f-47c6-d3e1-4d4803c5f9ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " essay (InputLayer)             [(None, 339)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 339, 100)     4727600     ['essay[0][0]']                  \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 339, 16)      7488        ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " prefix (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " state (InputLayer)             [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " cat (InputLayer)               [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " subcat (InputLayer)            [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " grade (InputLayer)             [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 339, 16)     64          ['lstm_1[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 1, 10)        70          ['prefix[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 1, 10)        530         ['state[0][0]']                  \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 3, 10)        110         ['cat[0][0]']                    \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)       (None, 3, 10)        320         ['subcat[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 1, 10)        60          ['grade[0][0]']                  \n",
      "                                                                                                  \n",
      " numeric (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 5424)         0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 10)           0           ['embedding_11[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 10)           0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 30)           0           ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 30)           0           ['embedding_10[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 10)           0           ['embedding_8[0][0]']            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           96          ['numeric[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 5546)         0           ['flatten_6[0][0]',              \n",
      "                                                                  'flatten_11[0][0]',             \n",
      "                                                                  'flatten_7[0][0]',              \n",
      "                                                                  'flatten_9[0][0]',              \n",
      "                                                                  'flatten_10[0][0]',             \n",
      "                                                                  'flatten_8[0][0]',              \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           355008      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 32)           2080        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32)           0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 16)           528         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2)            34          ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,093,988\n",
      "Trainable params: 366,356\n",
      "Non-trainable params: 4,727,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Writing code for all the inputs\n",
    "\n",
    "text_input=tf.keras.Input(shape=(padded_essay_train.shape[1]), name= 'essay')\n",
    "state_input=tf.keras.Input(shape=(padded_state_train.shape[1]),name='state')\n",
    "prefix_input=tf.keras.Input(shape=(padded_prefix_train.shape[1]), name='prefix')\n",
    "cat_input=tf.keras.Input(shape=(padded_cat_train.shape[1]),name='cat')\n",
    "subcat_input=tf.keras.Input(shape=(padded_subcat_train.shape[1]),name='subcat')\n",
    "grade_input=tf.keras.Input(shape=(padded_grade_train.shape[1]),name='grade')\n",
    "numeric_input=tf.keras.Input(shape=(numerical_train.shape[1]),name='numeric')\n",
    "\n",
    "#Text Processing Networks\n",
    "embedded_output_essay=tf.keras.layers.Embedding(input_dim=vocab_size ,output_dim=100,input_length=padded_essay_train.shape[1], input_shape=text_input.shape, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False )(text_input)\n",
    "\n",
    "lstm_output_essay=tf.keras.layers.LSTM(16,return_sequences=True)(embedded_output_essay)\n",
    "lstm_output_essay=tf.keras.layers.BatchNormalization()(lstm_output_essay)\n",
    "text_input_1=tf.keras.layers.Flatten()(lstm_output_essay)\n",
    "#,kernel_initializer=tf.keras.initializers.HeNormal(20) \n",
    "#,kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001)\n",
    "\n",
    "#State Input\n",
    "input_dim_state=max(state_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_state=tf.keras.layers.Embedding(input_dim=input_dim_state,output_dim=10,input_length=padded_state_train.shape[1])(state_input)\n",
    "state_input_1=tf.keras.layers.Flatten()(embedded_output_state)\n",
    "\n",
    "#Grade Input\n",
    "input_dim_grade=max(grade_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_grade=tf.keras.layers.Embedding(input_dim=input_dim_grade,output_dim=10,input_length=padded_grade_train.shape[1])(grade_input)\n",
    "grade_input_1=tf.keras.layers.Flatten()(embedded_output_grade)\n",
    "\n",
    "#category Input\n",
    "input_dim_cat=max(cat_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_cat=tf.keras.layers.Embedding(input_dim=input_dim_cat,output_dim=10,input_length=padded_cat_train.shape[1])(cat_input)\n",
    "cat_input_1=tf.keras.layers.Flatten()(embedded_output_cat)\n",
    "\n",
    "#subcategory Input\n",
    "input_dim_subcat=max(subcat_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_subcat=tf.keras.layers.Embedding(input_dim=input_dim_subcat,output_dim=10,input_length=padded_subcat_train.shape[1])(subcat_input)\n",
    "subcat_input_1=tf.keras.layers.Flatten()(embedded_output_subcat)\n",
    "\n",
    "#subcategory Input\n",
    "input_dim_prefix=max(prefix_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_prefix=tf.keras.layers.Embedding(input_dim=input_dim_prefix,output_dim=10,input_length=padded_prefix_train.shape[1])(prefix_input)\n",
    "prefix_input_1=tf.keras.layers.Flatten()(embedded_output_prefix)\n",
    "\n",
    "#numerical input\n",
    "numeric_input_1=tf.keras.layers.Dense(32,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(30))(numeric_input)\n",
    "\n",
    "#Concatenating the inputs\n",
    "concatenated_input=tf.keras.layers.concatenate([text_input_1,prefix_input_1,state_input_1,cat_input_1,subcat_input_1,grade_input_1,numeric_input_1],axis=1)\n",
    "\n",
    "#Layers after the concatenating layers\n",
    "#,kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001)\n",
    "concatenated_input=tf.keras.layers.Dense(64,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(50))(concatenated_input)\n",
    "concatenated_input_1=tf.keras.layers.Dropout(dropout)(concatenated_input)\n",
    "concatenated_input_2=tf.keras.layers.Dense(32,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(60))(concatenated_input_1)\n",
    "concatenated_input_3=tf.keras.layers.Dropout(dropout)(concatenated_input_2)\n",
    "concatenated_input_4=tf.keras.layers.Dense(16,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(20))(concatenated_input_3)\n",
    "output_layer=tf.keras.layers.Dense(2,activation='softmax')(concatenated_input_4)\n",
    "\n",
    "model=tf.keras.Model(inputs={'essay':text_input,'prefix':prefix_input, 'state':state_input, 'cat':cat_input, 'subcat':subcat_input, 'grade':grade_input, 'numeric':numeric_input},outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sdh4yihY-8EJ"
   },
   "source": [
    "## 1.5 Compiling and fittng your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYWneflx-8EK"
   },
   "outputs": [],
   "source": [
    "#define custom auc as metric , do not use tf.keras.metrics\n",
    "# https://stackoverflow.com/a/46844409 - custom AUC reference 1\n",
    "# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807  - custom AUC reference 2\n",
    "# compile and fit your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VArSbTjBWOCJ",
    "outputId": "66ad40da-84c5-4d64-cde6-cffc683c734e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network\n",
    "print(np.any(np.isnan(padded_prefix_train)))\n",
    "print(np.any(np.isnan(padded_state_train)))\n",
    "print(np.any(np.isnan(padded_grade_train)))\n",
    "print(np.any(np.isnan(padded_cat_train)))\n",
    "print(np.any(np.isnan(padded_subcat_train)))\n",
    "print(np.any(np.isnan(padded_essay_train)))\n",
    "print(np.any(np.isnan(numerical_train)))\n",
    "print(np.any(np.isnan(embedding_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpFHsDY-z6G0",
    "outputId": "52f933ed-454c-4a69-f29b-d78b53394b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network\n",
    "print(np.any(np.isinf(padded_prefix_train)))\n",
    "print(np.any(np.isinf(padded_state_train)))\n",
    "print(np.any(np.isinf(padded_grade_train)))\n",
    "print(np.any(np.isinf(padded_cat_train)))\n",
    "print(np.any(np.isinf(padded_subcat_train)))\n",
    "print(np.any(np.isinf(padded_essay_train)))\n",
    "print(np.any(np.isinf(numerical_train)))\n",
    "print(np.any(np.isinf(embedding_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fir2IuMVs4A",
    "outputId": "171f1a31-694c-49af-8059-54de5e4828a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.all(np.isfinite(padded_prefix_train)))\n",
    "print(np.all(np.isfinite(padded_state_train)))\n",
    "print(np.all(np.isfinite(padded_grade_train)))\n",
    "print(np.all(np.isfinite(padded_cat_train)))\n",
    "print(np.all(np.isfinite(padded_subcat_train)))\n",
    "print(np.all(np.isfinite(padded_essay_train)))\n",
    "print(np.all(np.isfinite(numerical_train)))\n",
    "print(np.all(np.isfinite(embedding_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZE52kLsWIq3",
    "outputId": "62d9616e-3dc1-4bd0-c55e-7adf99ab891c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(padded_prefix_train))\n",
    "print(type(padded_state_train))\n",
    "print(type(padded_grade_train))\n",
    "print(type(padded_cat_train))\n",
    "print(type(padded_subcat_train))\n",
    "print(type(padded_essay_train))\n",
    "print(type(numerical_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwvNlllm-8EK",
    "outputId": "560ade06-9bdb-42a8-e225-07d6476b5f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1092/1093 [============================>.] - ETA: 0s - loss: 0.7013 - auc: 0.6135 - accuracy: 0.5223\n",
      "Epoch 1: val_auc improved from -inf to 0.71418, saving model to /content/drive/MyDrive/Assignment 24/f_model.hdf5\n",
      "1093/1093 [==============================] - 41s 35ms/step - loss: 0.7013 - auc: 0.6136 - accuracy: 0.5224 - val_loss: 0.6979 - val_auc: 0.7142 - val_accuracy: 0.6446\n",
      "Epoch 2/6\n",
      "1092/1093 [============================>.] - ETA: 0s - loss: 0.6289 - auc: 0.7168 - accuracy: 0.7253\n",
      "Epoch 2: val_auc improved from 0.71418 to 0.72262, saving model to /content/drive/MyDrive/Assignment 24/f_model.hdf5\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6288 - auc: 0.7170 - accuracy: 0.7253 - val_loss: 0.6251 - val_auc: 0.7226 - val_accuracy: 0.6931\n",
      "Epoch 3/6\n",
      "1093/1093 [==============================] - ETA: 0s - loss: 0.6130 - auc: 0.7404 - accuracy: 0.7387\n",
      "Epoch 3: val_auc improved from 0.72262 to 0.73381, saving model to /content/drive/MyDrive/Assignment 24/f_model.hdf5\n",
      "1093/1093 [==============================] - 38s 35ms/step - loss: 0.6130 - auc: 0.7404 - accuracy: 0.7387 - val_loss: 0.5502 - val_auc: 0.7338 - val_accuracy: 0.7842\n",
      "Epoch 4/6\n",
      "1092/1093 [============================>.] - ETA: 0s - loss: 0.5972 - auc: 0.7593 - accuracy: 0.7355\n",
      "Epoch 4: val_auc improved from 0.73381 to 0.73704, saving model to /content/drive/MyDrive/Assignment 24/f_model.hdf5\n",
      "1093/1093 [==============================] - 38s 35ms/step - loss: 0.5973 - auc: 0.7593 - accuracy: 0.7355 - val_loss: 0.8060 - val_auc: 0.7370 - val_accuracy: 0.4883\n",
      "Epoch 5/6\n",
      "1092/1093 [============================>.] - ETA: 0s - loss: 0.5784 - auc: 0.7814 - accuracy: 0.7509\n",
      "Epoch 5: val_auc did not improve from 0.73704\n",
      "1093/1093 [==============================] - 31s 28ms/step - loss: 0.5784 - auc: 0.7814 - accuracy: 0.7508 - val_loss: 0.6068 - val_auc: 0.7005 - val_accuracy: 0.7073\n",
      "Epoch 6/6\n",
      "1091/1093 [============================>.] - ETA: 0s - loss: 0.5521 - auc: 0.8108 - accuracy: 0.7685\n",
      "Epoch 6: val_auc did not improve from 0.73704\n",
      "1093/1093 [==============================] - 33s 30ms/step - loss: 0.5520 - auc: 0.8108 - accuracy: 0.7686 - val_loss: 0.4920 - val_auc: 0.7240 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d7ff74820>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "\n",
    "paths='/content/drive/MyDrive/Assignment 24/f_model.hdf5'\n",
    "\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(paths, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.003, beta_1 = 1e-4), loss='categorical_crossentropy', metrics=[auc,'accuracy'])\n",
    "\n",
    "logs='/content/drive/MyDrive/Assignment 24/logs/model_1'\n",
    "\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=logs)\n",
    "\n",
    "\n",
    "model.fit({'essay':padded_essay_train,'prefix':padded_prefix_train, 'state':padded_state_train, 'cat':padded_cat_train, 'subcat':padded_subcat_train, 'grade':padded_grade_train, 'numeric':numerical_train}, y_train,\n",
    "          epochs=6, validation_data=({'essay':padded_essay_cv,'prefix':padded_prefix_cv, 'state':padded_state_cv, 'cat':padded_cat_cv, 'subcat':padded_subcat_cv, 'grade':padded_grade_cv, 'numeric':numerical_cv}, y_cv)\n",
    "          , verbose=1,callbacks=[tensorboard,checkpoint_save], batch_size=64,class_weight={0:class_weight[0],1:class_weight[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "grWoA_PX46IA"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/MyDrive/Assignment 24/f_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8R49H5KY3wK",
    "outputId": "f1b0cae9-2a88-4dae-bd28-5ee0ee59192c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21850/21850 [==============================] - 197s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_pred=model.predict({'essay':padded_essay_test,'prefix':padded_prefix_test, 'state':padded_state_test, 'cat':padded_cat_test, 'subcat':padded_subcat_test, 'grade':padded_grade_test, 'numeric':numerical_test}, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agUNmvszY33K",
    "outputId": "2cb5bd68-c4bb-4cc6-826d-61c47a59baa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.743522948391162>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(y_test[:,1],y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oVCkDWViNVD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrvre1MQ-8EL"
   },
   "source": [
    "# <font color='red'> Model-2 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hoq1CoJk-8EL"
   },
   "source": [
    "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNXaDKAN-8EL"
   },
   "source": [
    "<pre>\n",
    "1. Fit TF-IDF vectorizer on the Train data <br>\n",
    "2. Get the idf value for each word we have in the train data. Please go through <a  href='https://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer'>this</a><br>\n",
    "\n",
    "3. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very \n",
    "frequent words and very very rare words don't give much information.\n",
    "Hint - A preferable IDF range is 2-11 for model 2. <br>\n",
    "4.Remove the low idf value and high idf value words from the train and test data. You can go through each of the\n",
    "sentence of train and test data and include only those features(words) which are present in the defined IDF range.\n",
    "5. Perform tokenization on the modified text data same as you have done for previous model.\n",
    "6. Create embedding matrix for model 2 and then use the rest of the features similar to previous model.\n",
    "7. Define the model, compile and fit the model.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUOGN2OrbuV3",
    "outputId": "6c68096f-3cc5-468e-ec5e-d08a5730788c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer=TfidfVectorizer()\n",
    "\n",
    "tf_vectorizer.fit(x_train['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4MPOtf7-8EM"
   },
   "outputs": [],
   "source": [
    "idf_scores=tf_vectorizer.idf_\n",
    "vocabulary=tf_vectorizer.vocabulary_.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "9GsQHeFqWc1G",
    "outputId": "b3bca8fc-02d4-4f2b-aec3-568766ebbb9d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_figwidth(10)\n",
    "f.set_figheight(10)\n",
    "\n",
    "plt.title('Box Plot on IDF')\n",
    "plt.xlabel('IDF of Words')\n",
    "plt.ylabel('IDF scores')\n",
    "\n",
    "sns.boxplot(y=tf_vectorizer.idf_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3rzGRt9z1R6",
    "outputId": "283c148d-20a0-4a64-cdea-06aa06cf9169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100th Percentile is  11.4619455276077\n",
      "The 1th Percentile is  4.052097932091764\n"
     ]
    }
   ],
   "source": [
    "print('The 100th Percentile is ',np.percentile((tf_vectorizer.idf_),100))\n",
    "print('The 1th Percentile is ',np.percentile((tf_vectorizer.idf_),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0U2l9O-HyWJb",
    "outputId": "d679e747-fae5-4dee-e2b7-b0d9a24694e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 percentile is 1.0081857021393015\n",
      "The 10 percentile is 7.454612342375229\n",
      "The 20 percentile is 8.896996170146164\n",
      "The 30 percentile is 9.8525076151736\n",
      "The 40 percentile is 10.545654795733544\n",
      "The 50 percentile is 11.056480419499536\n",
      "The 60 percentile is 11.056480419499536\n",
      "The 70 percentile is 11.4619455276077\n",
      "The 80 percentile is 11.4619455276077\n",
      "The 90 percentile is 11.4619455276077\n",
      "The 100 percentile is 11.4619455276077\n"
     ]
    }
   ],
   "source": [
    "for values in range(0,101,10):\n",
    "  print('The {} percentile is {}'.format(values,np.percentile((tf_vectorizer.idf_),values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GqiRLflyuW9",
    "outputId": "af29ae32-c42e-46af-9609-db93beb1ef36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0.0 percentile is 1.0081857021393015\n",
      "The 0.1 percentile is 2.3385042392073965\n",
      "The 0.2 percentile is 2.6897320086784178\n",
      "The 0.30000000000000004 percentile is 3.0122203495596924\n",
      "The 0.4 percentile is 3.2431664962310935\n",
      "The 0.5 percentile is 3.437652634216872\n",
      "The 0.6000000000000001 percentile is 3.5726829770066746\n",
      "The 0.7000000000000001 percentile is 3.704750047694964\n",
      "The 0.8 percentile is 3.83309200547498\n",
      "The 0.9 percentile is 3.9619499127083895\n"
     ]
    }
   ],
   "source": [
    "for values in np.arange(0,1,0.1):\n",
    "  print('The {} percentile is {}'.format(values,np.percentile((tf_vectorizer.idf_),values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgToG0YaXbs7",
    "outputId": "d7c214c2-2a0e-4b30-8359-5654eb69ff45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 90 percentile is 11.4619455276077\n",
      "The 91 percentile is 11.4619455276077\n",
      "The 92 percentile is 11.4619455276077\n",
      "The 93 percentile is 11.4619455276077\n",
      "The 94 percentile is 11.4619455276077\n",
      "The 95 percentile is 11.4619455276077\n",
      "The 96 percentile is 11.4619455276077\n",
      "The 97 percentile is 11.4619455276077\n",
      "The 98 percentile is 11.4619455276077\n",
      "The 99 percentile is 11.4619455276077\n"
     ]
    }
   ],
   "source": [
    "for values in np.arange(90,100,1):\n",
    "  print('The {} percentile is {}'.format(values,np.percentile((tf_vectorizer.idf_),values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pllK1zox-K9"
   },
   "outputs": [],
   "source": [
    "#The model is not performing well with the threshold as 6.2 which is the IQR\n",
    "#Setting the threshold as 4.05\n",
    "#because the values above 50% have a idf value of 11, hence removing values above idf value of 11, and trying how the model performance is affected\n",
    "#At t=11 that is removing values above 12, the score is 0.735\n",
    "t=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmP6eqRg-8EM",
    "outputId": "1103ef0b-3050-4f55-9706-37edc84fb0bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47554/47554 [03:11<00:00, 248.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tf_dict=dict()\n",
    "threshold=t\n",
    "for words, index in tqdm(list(vocabulary)):\n",
    "  if list(idf_scores)[index]>t:\n",
    "      tf_dict[words]=list(idf_scores)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VL0a8nXm2iQ",
    "outputId": "b4be7da2-b112-456d-f5ad-13c550b8e379"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47529it [00:00, 1721497.01it/s]\n"
     ]
    }
   ],
   "source": [
    "new_word_index=dict()\n",
    "for i,word in tqdm(enumerate(list(tf_dict.keys()))):\n",
    "  new_word_index[word]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "as8Pnvn1dFcF"
   },
   "outputs": [],
   "source": [
    "tokenizer_model_2=tokenizer\n",
    "tokenizer_model_2.word_index=new_word_index\n",
    "tokenizer_model_2.word_index[tokenizer_model_2.oov_token] = max(new_word_index.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpE-2_adcPWt",
    "outputId": "9d53790f-0381-4e2e-d0ac-2e43619aaa96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47531\n"
     ]
    }
   ],
   "source": [
    "vocab_size_model_2=len(tokenizer_model_2.word_index.values())+1\n",
    "print(vocab_size_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkjZDYQKqpif",
    "outputId": "81bbe4bc-0bce-4c46-c68e-23928ce91c5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47530/47530 [00:00<00:00, 541539.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#Creating embedding matrix\n",
    "embedding_matrix_model_2 = np.zeros((vocab_size_model_2, 100))\n",
    "for word, i in tqdm(tokenizer_model_2.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_model_2[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFV-CqPVxA9N",
    "outputId": "81cdb9a9-d6e6-46fc-9cab-e45e2df748d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47531, 100)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2jiZvXbdcHJ"
   },
   "outputs": [],
   "source": [
    "essay_train_model_2=tokenizer_model_2.texts_to_sequences(x_train['essay'])\n",
    "essay_test_model_2=tokenizer_model_2.texts_to_sequences(x_test['essay'])\n",
    "essay_cv_model_2=tokenizer_model_2.texts_to_sequences(x_cv['essay'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cVwH4BDrpUg",
    "outputId": "fa2fac9f-fbc0-4d41-e4ce-ad5435a34630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n"
     ]
    }
   ],
   "source": [
    "max_length_model_2=max([len(i) for i in essay_train_model_2])\n",
    "print(max_length_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttx19R6Frd91"
   },
   "outputs": [],
   "source": [
    "padded_essay_train_model_2=tf.keras.utils.pad_sequences(essay_train_model_2,maxlen=max_length_model_2,padding='post',truncating='post')\n",
    "padded_essay_test_model_2=tf.keras.utils.pad_sequences(essay_test_model_2,maxlen=max_length_model_2,padding='post',truncating='post')\n",
    "padded_essay_cv_model_2=tf.keras.utils.pad_sequences(essay_cv_model_2,maxlen=max_length_model_2,padding='post',truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1Gud32ntXZk",
    "outputId": "dcf92136-e6e7-4067-acbf-68b5de29e181"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47530"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_model_2.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70X5rjTutjV-",
    "outputId": "0812fb5d-2c44-4500-f2ca-e1642e458646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47531, 100)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnwRxLdQkp24"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV0q6_59lWJZ"
   },
   "outputs": [],
   "source": [
    "activation_function='relu'\n",
    "dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFMIsJFRsgHs",
    "outputId": "757f42e9-ee98-4b27-d3f9-dddff61c1c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " essay (InputLayer)             [(None, 333)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_55 (Embedding)       (None, 333, 100)     4753100     ['essay[0][0]']                  \n",
      "                                                                                                  \n",
      " prefix (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " state (InputLayer)             [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " cat (InputLayer)               [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " subcat (InputLayer)            [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " grade (InputLayer)             [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " lstm_10 (LSTM)                 (None, 333, 32)      17024       ['embedding_55[0][0]']           \n",
      "                                                                                                  \n",
      " embedding_60 (Embedding)       (None, 1, 10)        70          ['prefix[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding_56 (Embedding)       (None, 1, 10)        530         ['state[0][0]']                  \n",
      "                                                                                                  \n",
      " embedding_58 (Embedding)       (None, 3, 10)        110         ['cat[0][0]']                    \n",
      "                                                                                                  \n",
      " embedding_59 (Embedding)       (None, 3, 10)        320         ['subcat[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding_57 (Embedding)       (None, 1, 10)        60          ['grade[0][0]']                  \n",
      "                                                                                                  \n",
      " numeric (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten_56 (Flatten)           (None, 10656)        0           ['lstm_10[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_61 (Flatten)           (None, 10)           0           ['embedding_60[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_57 (Flatten)           (None, 10)           0           ['embedding_56[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_59 (Flatten)           (None, 30)           0           ['embedding_58[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_60 (Flatten)           (None, 30)           0           ['embedding_59[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_58 (Flatten)           (None, 10)           0           ['embedding_57[0][0]']           \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 32)           96          ['numeric[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 10778)        0           ['flatten_56[0][0]',             \n",
      "                                                                  'flatten_61[0][0]',             \n",
      "                                                                  'flatten_57[0][0]',             \n",
      "                                                                  'flatten_59[0][0]',             \n",
      "                                                                  'flatten_60[0][0]',             \n",
      "                                                                  'flatten_58[0][0]',             \n",
      "                                                                  'dense_49[0][0]']               \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 64)           689856      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 64)           0           ['dense_50[0][0]']               \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 32)           2080        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 32)           0           ['dense_51[0][0]']               \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 16)           528         ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 2)            34          ['dense_52[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,463,808\n",
      "Trainable params: 710,708\n",
      "Non-trainable params: 4,753,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Writing code for all the inputs\n",
    "\n",
    "text_input=tf.keras.Input(shape=(padded_essay_train_model_2.shape[1]), name= 'essay')\n",
    "state_input=tf.keras.Input(shape=(padded_state_train.shape[1]),name='state')\n",
    "prefix_input=tf.keras.Input(shape=(padded_prefix_train.shape[1]), name='prefix')\n",
    "cat_input=tf.keras.Input(shape=(padded_cat_train.shape[1]),name='cat')\n",
    "subcat_input=tf.keras.Input(shape=(padded_subcat_train.shape[1]),name='subcat')\n",
    "grade_input=tf.keras.Input(shape=(padded_grade_train.shape[1]),name='grade')\n",
    "numeric_input=tf.keras.Input(shape=(numerical_train.shape[1]),name='numeric')\n",
    "\n",
    "#Text Processing Networks\n",
    "\n",
    "embedded_output_essay=tf.keras.layers.Embedding(input_dim=vocab_size_model_2 ,output_dim=100,input_length=padded_essay_train_model_2.shape[1], input_shape=text_input.shape, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_model_2),trainable=False )(text_input)\n",
    "\n",
    "lstm_output_essay=tf.keras.layers.LSTM(32,return_sequences=True,kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001))(embedded_output_essay)\n",
    "text_input_1=tf.keras.layers.Flatten()(lstm_output_essay)\n",
    "#,kernel_regularizer='l1_l2'\n",
    "#State Input\n",
    "input_dim_state=max(state_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_state=tf.keras.layers.Embedding(input_dim=input_dim_state,output_dim=10,input_length=padded_state_train.shape[1])(state_input)\n",
    "state_input_1=tf.keras.layers.Flatten()(embedded_output_state)\n",
    "\n",
    "#Grade Input\n",
    "input_dim_grade=max(grade_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_grade=tf.keras.layers.Embedding(input_dim=input_dim_grade,output_dim=10,input_length=padded_grade_train.shape[1])(grade_input)\n",
    "grade_input_1=tf.keras.layers.Flatten()(embedded_output_grade)\n",
    "\n",
    "#category Input\n",
    "input_dim_cat=max(cat_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_cat=tf.keras.layers.Embedding(input_dim=input_dim_cat,output_dim=10,input_length=padded_cat_train.shape[1])(cat_input)\n",
    "cat_input_1=tf.keras.layers.Flatten()(embedded_output_cat)\n",
    "\n",
    "#subcategory Input\n",
    "input_dim_subcat=max(subcat_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_subcat=tf.keras.layers.Embedding(input_dim=input_dim_subcat,output_dim=10,input_length=padded_subcat_train.shape[1])(subcat_input)\n",
    "subcat_input_1=tf.keras.layers.Flatten()(embedded_output_subcat)\n",
    "\n",
    "#subcategory Input\n",
    "input_dim_prefix=max(prefix_tokenizer.word_index.values())+1\n",
    "\n",
    "embedded_output_prefix=tf.keras.layers.Embedding(input_dim=input_dim_prefix,output_dim=10,input_length=padded_prefix_train.shape[1])(prefix_input)\n",
    "prefix_input_1=tf.keras.layers.Flatten()(embedded_output_prefix)\n",
    "\n",
    "#numerical input\n",
    "numeric_input_1=tf.keras.layers.Dense(32,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(30))(numeric_input)\n",
    "\n",
    "#Concatenating the inputs\n",
    "concatenated_input=tf.keras.layers.concatenate([text_input_1,prefix_input_1,state_input_1,cat_input_1,subcat_input_1,grade_input_1,numeric_input_1],axis=1)\n",
    "\n",
    "#Layers after the concatenating layers\n",
    "concatenated_input=tf.keras.layers.Dense(64,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(50),kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001))(concatenated_input)\n",
    "concatenated_input_1=tf.keras.layers.Dropout(dropout)(concatenated_input)\n",
    "concatenated_input_2=tf.keras.layers.Dense(32,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(60),kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001))(concatenated_input_1)\n",
    "concatenated_input_3=tf.keras.layers.Dropout(dropout)(concatenated_input_2)\n",
    "concatenated_input_4=tf.keras.layers.Dense(16,activation=activation_function,kernel_initializer=tf.keras.initializers.HeNormal(20),kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001))(concatenated_input_3)\n",
    "output_layer=tf.keras.layers.Dense(2,activation='softmax')(concatenated_input_4)\n",
    "\n",
    "model_2=tf.keras.Model(inputs={'essay':text_input,'prefix':prefix_input, 'state':state_input, 'cat':cat_input, 'subcat':subcat_input, 'grade':grade_input, 'numeric':numeric_input},outputs=output_layer)\n",
    "model_2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya8hTPz1r0Yl",
    "outputId": "a7b400e3-3f63-4200-f8f5-97b07832a3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1092/1093 [============================>.] - ETA: 0s - loss: 0.7027 - auc: 0.6404 - accuracy: 0.6114\n",
      "Epoch 1: val_auc improved from -inf to 0.71530, saving model to /content/drive/MyDrive/Assignment 24/f_model_2.hdf5\n",
      "1093/1093 [==============================] - 33s 28ms/step - loss: 0.7027 - auc: 0.6405 - accuracy: 0.6115 - val_loss: 0.6143 - val_auc: 0.7153 - val_accuracy: 0.7766\n",
      "Epoch 2/10\n",
      "1093/1093 [==============================] - ETA: 0s - loss: 0.6582 - auc: 0.7214 - accuracy: 0.7063\n",
      "Epoch 2: val_auc improved from 0.71530 to 0.73022, saving model to /content/drive/MyDrive/Assignment 24/f_model_2.hdf5\n",
      "1093/1093 [==============================] - 30s 28ms/step - loss: 0.6582 - auc: 0.7214 - accuracy: 0.7063 - val_loss: 0.6394 - val_auc: 0.7302 - val_accuracy: 0.7761\n",
      "Epoch 3/10\n",
      "1091/1093 [============================>.] - ETA: 0s - loss: 0.6446 - auc: 0.7384 - accuracy: 0.7229\n",
      "Epoch 3: val_auc improved from 0.73022 to 0.73459, saving model to /content/drive/MyDrive/Assignment 24/f_model_2.hdf5\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6446 - auc: 0.7382 - accuracy: 0.7229 - val_loss: 0.6558 - val_auc: 0.7346 - val_accuracy: 0.7839\n",
      "Epoch 4/10\n",
      "1091/1093 [============================>.] - ETA: 0s - loss: 0.6401 - auc: 0.7511 - accuracy: 0.7337\n",
      "Epoch 4: val_auc improved from 0.73459 to 0.73512, saving model to /content/drive/MyDrive/Assignment 24/f_model_2.hdf5\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6403 - auc: 0.7511 - accuracy: 0.7336 - val_loss: 0.7388 - val_auc: 0.7351 - val_accuracy: 0.6647\n",
      "Epoch 5/10\n",
      "1093/1093 [==============================] - ETA: 0s - loss: 0.6392 - auc: 0.7681 - accuracy: 0.7364\n",
      "Epoch 5: val_auc did not improve from 0.73512\n",
      "1093/1093 [==============================] - 33s 31ms/step - loss: 0.6392 - auc: 0.7681 - accuracy: 0.7364 - val_loss: 0.6577 - val_auc: 0.7341 - val_accuracy: 0.7659\n",
      "Epoch 6/10\n",
      "1092/1093 [============================>.] - ETA: 0s - loss: 0.6316 - auc: 0.7861 - accuracy: 0.7511\n",
      "Epoch 6: val_auc did not improve from 0.73512\n",
      "1093/1093 [==============================] - 29s 26ms/step - loss: 0.6315 - auc: 0.7862 - accuracy: 0.7511 - val_loss: 0.7315 - val_auc: 0.7320 - val_accuracy: 0.6961\n",
      "Epoch 7/10\n",
      "1091/1093 [============================>.] - ETA: 0s - loss: 0.6213 - auc: 0.8114 - accuracy: 0.7660\n",
      "Epoch 7: val_auc did not improve from 0.73512\n",
      "1093/1093 [==============================] - 25s 22ms/step - loss: 0.6212 - auc: 0.8115 - accuracy: 0.7661 - val_loss: 0.6140 - val_auc: 0.7241 - val_accuracy: 0.7860\n",
      "Epoch 8/10\n",
      "1091/1093 [============================>.] - ETA: 0s - loss: 0.6090 - auc: 0.8369 - accuracy: 0.7781\n",
      "Epoch 8: val_auc did not improve from 0.73512\n",
      "1093/1093 [==============================] - 23s 21ms/step - loss: 0.6090 - auc: 0.8368 - accuracy: 0.7781 - val_loss: 0.6462 - val_auc: 0.7187 - val_accuracy: 0.7809\n",
      "Epoch 9/10\n",
      "1093/1093 [==============================] - ETA: 0s - loss: 0.5904 - auc: 0.8600 - accuracy: 0.7955\n",
      "Epoch 9: val_auc did not improve from 0.73512\n",
      "1093/1093 [==============================] - 25s 23ms/step - loss: 0.5904 - auc: 0.8600 - accuracy: 0.7955 - val_loss: 0.6707 - val_auc: 0.7055 - val_accuracy: 0.7545\n",
      "Epoch 10/10\n",
      "1092/1093 [============================>.] - ETA: 0s - loss: 0.5750 - auc: 0.8798 - accuracy: 0.8054\n",
      "Epoch 10: val_auc did not improve from 0.73512\n",
      "1093/1093 [==============================] - 25s 23ms/step - loss: 0.5750 - auc: 0.8799 - accuracy: 0.8054 - val_loss: 0.6785 - val_auc: 0.7005 - val_accuracy: 0.7503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8682fec9a0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(optimizer=tf.keras.optimizers.Adam(0.001, beta_1 = 1e-4), loss='categorical_crossentropy', metrics=[auc, 'accuracy'])\n",
    "\n",
    "paths='/content/drive/MyDrive/Assignment 24/f_model_2.hdf5'\n",
    "\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(paths, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "logs='/content/drive/MyDrive/Assignment 24/logs/model_2'\n",
    "\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=logs)\n",
    "\n",
    "model_2.fit({'essay':padded_essay_train_model_2,'prefix':padded_prefix_train, 'state':padded_state_train, 'cat':padded_cat_train, 'subcat':padded_subcat_train, 'grade':padded_grade_train, 'numeric':numerical_train}, y_train,\n",
    "          epochs=10, validation_data=({'essay':padded_essay_cv_model_2,'prefix':padded_prefix_cv, 'state':padded_state_cv, 'cat':padded_cat_cv, 'subcat':padded_subcat_cv, 'grade':padded_grade_cv, 'numeric':numerical_cv}, y_cv)\n",
    "          , verbose=1,callbacks=[checkpoint_save,tensorboard], batch_size=64,class_weight={0:class_weight[0],1:class_weight[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tALdLyn-alxm"
   },
   "outputs": [],
   "source": [
    "model_2.load_weights('/content/drive/MyDrive/Assignment 24/f_model_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkKQMrcVsAUp",
    "outputId": "0a682994-9699-4efe-80db-bb1995092b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21850/21850 [==============================] - 138s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_model_2=model_2.predict({'essay':padded_essay_test_model_2,'prefix':padded_prefix_test, 'state':padded_state_test, 'cat':padded_cat_test, 'subcat':padded_subcat_test, 'grade':padded_grade_test, 'numeric':numerical_test}, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XaOI9TP4bKVT",
    "outputId": "6380dfd7-cb00-4d9a-8d9a-e2399f12e3a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.7419146271016863>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(y_test[:,1],y_test_pred_model_2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VV32QUI-8EM"
   },
   "source": [
    "# <font color='red'> Model-3 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk8gLFLf-8EN"
   },
   "source": [
    "<img src='https://i.imgur.com/fkQ8nGo.png'>\n",
    "\n",
    "*   List item\n",
    "\n",
    "*   List item\n",
    "\n",
    "*   List item\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n",
    "\n",
    "*   List item\n",
    "\n",
    "\n",
    "*   List item\n",
    "\n",
    "\n",
    "*   List item\n",
    "\n",
    "\n",
    "ref: https://i.imgur.com/fkQ8nGo.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "0_-Xld-M-8EN",
    "outputId": "67164590-7adb-4a8c-d7d6-0c1e83615563"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-18ee6e3b-e61b-4086-a986-1b328883243d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37466</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>our students range kindergarten eighth grade t...</td>\n",
       "      <td>374.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18ee6e3b-e61b-4086-a986-1b328883243d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-18ee6e3b-e61b-4086-a986-1b328883243d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-18ee6e3b-e61b-4086-a986-1b328883243d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      school_state teacher_prefix project_grade_category  \\\n",
       "37466           ca            mrs             grades_3_5   \n",
       "\n",
       "       teacher_number_of_previously_posted_projects clean_categories  \\\n",
       "37466                                             1     specialneeds   \n",
       "\n",
       "      clean_subcategories                                              essay  \\\n",
       "37466        specialneeds  our students range kindergarten eighth grade t...   \n",
       "\n",
       "        price  \n",
       "37466  374.57  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this model you can use the text vectorized data from model1 \n",
    "#for other than text data consider the following steps\n",
    "# you have to perform one hot encoding of categorical features. You can use onehotencoder() or countvectorizer() for the same.\n",
    "# Stack up standardised numerical features and all the one hot encoded categorical features\n",
    "#the input to conv1d layer is 3d, you can convert your 2d data to 3d using np.newaxis\n",
    "# Note - deep learning models won't work with sparse features, you have to convert them to dense features before fitting in the model.\n",
    "x_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7RFVMS1-8EN",
    "outputId": "999ba73c-5de1-4db9-e84d-23b69e10dc0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69918, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "grade_vectorizer=CountVectorizer(binary=True)\n",
    "grade_vectorizer.fit(x_train['project_grade_category'])\n",
    "grade_train_model2=grade_vectorizer.transform(x_train['project_grade_category'])\n",
    "grade_test_model2=grade_vectorizer.transform(x_test['project_grade_category'])\n",
    "grade_cv_model2=grade_vectorizer.transform(x_cv['project_grade_category'])\n",
    "print(grade_train_model2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sv6-iRGj-8EN",
    "outputId": "70c81d62-3837-4079-a068-5f4d1f4c40b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69918, 51)\n"
     ]
    }
   ],
   "source": [
    "state_vectorizer=CountVectorizer(binary=True)\n",
    "state_vectorizer.fit(x_train['school_state'])\n",
    "state_train_model2=state_vectorizer.transform(x_train['school_state'])\n",
    "state_test_model2=state_vectorizer.transform(x_test['school_state'])\n",
    "state_cv_model2=state_vectorizer.transform(x_cv['school_state'])\n",
    "print(state_train_model2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxFYIPLCz4TM",
    "outputId": "cd26edf9-7a04-4fe1-9593-ec12e3c5b2e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69918, 5)\n"
     ]
    }
   ],
   "source": [
    "prefix_vectorizer=CountVectorizer(binary=True)\n",
    "prefix_vectorizer.fit(x_train['teacher_prefix'])\n",
    "prefix_train_model2=prefix_vectorizer.transform(x_train['teacher_prefix'])\n",
    "prefix_test_model2=prefix_vectorizer.transform(x_test['teacher_prefix'])\n",
    "prefix_cv_model2=prefix_vectorizer.transform(x_cv['teacher_prefix'])\n",
    "print(prefix_train_model2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bVWy5SUUz43J",
    "outputId": "c8a2bed5-b499-4632-e335-777191c0c614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69918, 9)\n"
     ]
    }
   ],
   "source": [
    "cat_vectorizer=CountVectorizer(binary=True)\n",
    "cat_vectorizer.fit(x_train['clean_categories'])\n",
    "cat_train_model2=cat_vectorizer.transform(x_train['clean_categories'])\n",
    "cat_test_model2=cat_vectorizer.transform(x_test['clean_categories'])\n",
    "cat_cv_model2=cat_vectorizer.transform(x_cv['clean_categories'])\n",
    "print(cat_train_model2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TSesvUYz5dc",
    "outputId": "25db69f9-2e1f-44fb-9c03-88cb199228fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69918, 4)\n"
     ]
    }
   ],
   "source": [
    "subcat_vectorizer=CountVectorizer(binary=True)\n",
    "subcat_vectorizer.fit(x_train['project_grade_category'])\n",
    "subcat_train_model2=subcat_vectorizer.transform(x_train['project_grade_category'])\n",
    "subcat_test_model2=subcat_vectorizer.transform(x_test['project_grade_category'])\n",
    "subcat_cv_model2=subcat_vectorizer.transform(x_cv['project_grade_category'])\n",
    "print(subcat_train_model2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3W5DSOC0kjI"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "categorical_train_data=hstack([subcat_train_model2,cat_train_model2,grade_train_model2,prefix_train_model2,state_train_model2]).toarray()\n",
    "categorical_test_data=hstack([subcat_test_model2,cat_test_model2,grade_test_model2,prefix_test_model2,state_test_model2]).toarray()\n",
    "categorical_cv_data=hstack([subcat_cv_model2,cat_cv_model2,grade_cv_model2,prefix_cv_model2,state_cv_model2]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ETfBfLymy4T"
   },
   "outputs": [],
   "source": [
    "stacked_cat_train=categorical_train_data[:,:,np.newaxis]\n",
    "stacked_cat_test=categorical_test_data[:,:,np.newaxis]\n",
    "stacked_cat_cv=categorical_cv_data[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAWQPjE2ceQQ",
    "outputId": "228d0fd4-3813-4d8c-8e98-7e0a7f86a93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69918, 73, 1)\n",
      "(69918, 1, 73)\n",
      "(1, 69918, 73)\n"
     ]
    }
   ],
   "source": [
    "print(categorical_train_data[:,:,np.newaxis].shape)\n",
    "print(categorical_train_data[:,np.newaxis].shape)\n",
    "print(categorical_train_data[np.newaxis,:].shape)\n",
    "\n",
    "#categorical_train_data[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXK7ElmkxxWg"
   },
   "outputs": [],
   "source": [
    "actvation_unit='relu'\n",
    "dropout=0.6 #0.3,0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shPKW8VD3y9R",
    "outputId": "4d9a6213-856b-48c1-dfef-d1598457c26d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " essay_feature (InputLayer)     [(None, 333)]        0           []                               \n",
      "                                                                                                  \n",
      " other (InputLayer)             [(None, 73, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding_24 (Embedding)       (None, 333, 100)     4759200     ['essay_feature[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 73, 64)       256         ['other[0][0]']                  \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 333, 32)      17024       ['embedding_24[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 37, 64)       12352       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_24 (Flatten)           (None, 10656)        0           ['lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_25 (Flatten)           (None, 2368)         0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 13024)        0           ['flatten_24[0][0]',             \n",
      "                                                                  'flatten_25[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 64)           833600      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64)           0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           2080        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 32)           0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 16)           528         ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 16)           0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 2)            34          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,625,074\n",
      "Trainable params: 865,874\n",
      "Non-trainable params: 4,759,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining the third model\n",
    "input_layer_essay=tf.keras.layers.Input(shape=(padded_essay_train.shape[1]), name='essay_feature')\n",
    "input_layer_other_features=tf.keras.layers.Input(shape=(categorical_train_data.shape[1],1), name='other')\n",
    "\n",
    "embedded_output_essay=tf.keras.layers.Embedding(input_dim=vocab_size ,output_dim=100,input_length=padded_essay_train.shape[1], input_shape=input_layer_essay.shape, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False )(input_layer_essay)\n",
    "\n",
    "lstm_output=tf.keras.layers.LSTM(32,return_sequences=True)(embedded_output_essay)\n",
    "\n",
    "#,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)\n",
    "\n",
    "text_input=tf.keras.layers.Flatten()(lstm_output)\n",
    "\n",
    "#Other features\n",
    "conv_1=tf.keras.layers.Conv1D(64,3,strides=1,padding='same', activation=actvation_unit,kernel_initializer='he_normal')(input_layer_other_features)\n",
    "conv_2=tf.keras.layers.Conv1D(64,3,strides=2,padding='same', activation=actvation_unit,kernel_initializer='he_normal')(conv_1)\n",
    "other_output=tf.keras.layers.Flatten()(conv_2)\n",
    "\n",
    "concatenated_input=tf.keras.layers.concatenate([text_input,other_output])\n",
    "\n",
    "dense_1=tf.keras.layers.Dense(64,actvation_unit,kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001))(concatenated_input)\n",
    "drop_1=tf.keras.layers.Dropout(0.6)(dense_1)\n",
    "\n",
    "dense_2=tf.keras.layers.Dense(32,actvation_unit,kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001))(drop_1)\n",
    "drop_2=tf.keras.layers.Dropout(0.5)(dense_2)\n",
    "\n",
    "dense_3=tf.keras.layers.Dense(16,actvation_unit,kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(l2=0.0001))(drop_2)\n",
    "drop_3=tf.keras.layers.Dropout(0.4)(dense_3)\n",
    "\n",
    "output= tf.keras.layers.Dense(2, 'softmax')(drop_3)\n",
    "\n",
    "model_3=tf.keras.Model(inputs={'essay_feature':input_layer_essay,'other':input_layer_other_features}, outputs=output)\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v30113S6q-MS"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XTNEVourTg8",
    "outputId": "cf709b26-9c4b-462f-f404-d9d0734098af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.7335 - auc: 0.5566\n",
      "Epoch 1: val_auc improved from -inf to 0.70351, saving model to /content/drive/MyDrive/Assignment 24/f_model_3.hdf5\n",
      "1166/1166 [==============================] - 34s 24ms/step - loss: 0.7335 - auc: 0.5566 - val_loss: 0.7345 - val_auc: 0.7035\n",
      "Epoch 2/5\n",
      "1164/1166 [============================>.] - ETA: 0s - loss: 0.6999 - auc: 0.6859\n",
      "Epoch 2: val_auc improved from 0.70351 to 0.72210, saving model to /content/drive/MyDrive/Assignment 24/f_model_3.hdf5\n",
      "1166/1166 [==============================] - 26s 22ms/step - loss: 0.6999 - auc: 0.6858 - val_loss: 0.6156 - val_auc: 0.7221\n",
      "Epoch 3/5\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.7009 - auc: 0.7060\n",
      "Epoch 3: val_auc improved from 0.72210 to 0.72541, saving model to /content/drive/MyDrive/Assignment 24/f_model_3.hdf5\n",
      "1166/1166 [==============================] - 35s 30ms/step - loss: 0.7009 - auc: 0.7060 - val_loss: 0.7205 - val_auc: 0.7254\n",
      "Epoch 4/5\n",
      "1164/1166 [============================>.] - ETA: 0s - loss: 0.7089 - auc: 0.7196\n",
      "Epoch 4: val_auc improved from 0.72541 to 0.72875, saving model to /content/drive/MyDrive/Assignment 24/f_model_3.hdf5\n",
      "1166/1166 [==============================] - 29s 25ms/step - loss: 0.7090 - auc: 0.7195 - val_loss: 0.7100 - val_auc: 0.7288\n",
      "Epoch 5/5\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.7200 - auc: 0.7288\n",
      "Epoch 5: val_auc improved from 0.72875 to 0.73203, saving model to /content/drive/MyDrive/Assignment 24/f_model_3.hdf5\n",
      "1166/1166 [==============================] - 31s 27ms/step - loss: 0.7200 - auc: 0.7288 - val_loss: 0.7165 - val_auc: 0.7320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8785e700a0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.003, beta_1 = 1e-4), loss='categorical_crossentropy', metrics=[auc])\n",
    "#lr=0.001 0.72\n",
    "\n",
    "paths='/content/drive/MyDrive/Assignment 24/f_model_3.hdf5'\n",
    "\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(paths, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "logs='/content/drive/MyDrive/Assignment 24/logs/model_3'\n",
    "\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=logs)\n",
    "\n",
    "model_3.fit({'essay_feature':padded_essay_train ,'other':stacked_cat_train},y_train, validation_data=({'essay_feature':padded_essay_cv ,'other':stacked_cat_cv},y_cv),\n",
    "          epochs=5 , verbose=1, batch_size=60,class_weight={0:class_weight[0],1:class_weight[1]},callbacks=[checkpoint_save,tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbCe2mwIsuyA",
    "outputId": "e7a4ffca-58a6-4229-f94f-48445046819b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.7237 - auc: 0.7448\n",
      "Epoch 1: val_auc did not improve from 0.73203\n",
      "1166/1166 [==============================] - 21s 18ms/step - loss: 0.7237 - auc: 0.7448 - val_loss: 0.6592 - val_auc: 0.7302\n",
      "Epoch 2/5\n",
      "1163/1166 [============================>.] - ETA: 0s - loss: 0.7258 - auc: 0.7564\n",
      "Epoch 2: val_auc did not improve from 0.73203\n",
      "1166/1166 [==============================] - 22s 19ms/step - loss: 0.7259 - auc: 0.7560 - val_loss: 0.7119 - val_auc: 0.7292\n",
      "Epoch 3/5\n",
      "1164/1166 [============================>.] - ETA: 0s - loss: 0.7283 - auc: 0.7700\n",
      "Epoch 3: val_auc did not improve from 0.73203\n",
      "1166/1166 [==============================] - 22s 19ms/step - loss: 0.7284 - auc: 0.7697 - val_loss: 0.7960 - val_auc: 0.7293\n",
      "Epoch 4/5\n",
      "1164/1166 [============================>.] - ETA: 0s - loss: 0.7302 - auc: 0.7773\n",
      "Epoch 4: val_auc improved from 0.73203 to 0.73502, saving model to /content/drive/MyDrive/Assignment 24/f_model_3.hdf5\n",
      "1166/1166 [==============================] - 26s 23ms/step - loss: 0.7302 - auc: 0.7773 - val_loss: 0.7386 - val_auc: 0.7350\n",
      "Epoch 5/5\n",
      "1163/1166 [============================>.] - ETA: 0s - loss: 0.7338 - auc: 0.7861\n",
      "Epoch 5: val_auc did not improve from 0.73502\n",
      "1166/1166 [==============================] - 22s 19ms/step - loss: 0.7336 - auc: 0.7859 - val_loss: 0.7319 - val_auc: 0.7297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f878a9b2c70>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit({'essay_feature':padded_essay_train ,'other':stacked_cat_train},y_train, validation_data=({'essay_feature':padded_essay_cv ,'other':stacked_cat_cv},y_cv),\n",
    "          epochs=5 , verbose=1, batch_size=60,class_weight={0:class_weight[0],1:class_weight[1]},callbacks=[checkpoint_save,tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaQha0xXab9-"
   },
   "outputs": [],
   "source": [
    "model_3.load_weights('/content/drive/MyDrive/Assignment 24/f_model_3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AeYMjvaAAh-x",
    "outputId": "a9868c3b-9152-44e4-b9f6-38b1aaeaf6a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21850/21850 [==============================] - 127s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_model_3=model_3.predict({'essay_feature':padded_essay_test ,'other':stacked_cat_test}, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cL6t3w4Ao_7",
    "outputId": "4541311c-0cba-47d5-a160-9bc4b581d121"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.7291166451483654>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(y_test[:,1],y_test_pred_model_3[:,1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
