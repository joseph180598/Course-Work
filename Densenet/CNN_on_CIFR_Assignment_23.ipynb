{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK3alCdFflQX"
   },
   "source": [
    "### CNN on CIFR Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHCYMwwXflQd"
   },
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use DropOut layers.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TLVcyNYKflQi"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803\n",
    "#We can read from the paper itself that the team got a test error of 3.64% on CIFAR10 Dataset using a 40 layer model with k=12.\n",
    "#The above article tells us how to check if the k value is 12.\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wWRcEY0eTqMd"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten,ReLU, concatenate,Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "#https://www.kaggle.com/code/thetenaciousguy/cnn-densenet-first3x3-l18\n",
    "#https://towardsdatascience.com/densenet-on-cifar10-d5651294a1a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QWcECdbJcQ8h"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQZbLy6BcpKp",
    "outputId": "2b6de447-97ce-442f-ed99-f873fee93d52"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "num_classes=10\n",
    "(X_train_1, y_train_1), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_cv, y_train, y_cv=train_test_split(X_train_1, y_train_1, test_size=0.2, stratify=y_train_1)\n",
    "\n",
    "X_test=X_test/255\n",
    "x_cv=x_cv/255\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) \n",
    "y_cv = tf.keras.utils.to_categorical(y_cv, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVjGA4UKiV_P",
    "outputId": "6177b6af-0b29-4538-f9b3-e1c0e401d926"
   },
   "outputs": [],
   "source": [
    "'''import pickle\n",
    "\n",
    "with open ('pickle_files/x_train.pkl', 'wb') as f:\n",
    "    f= pickle.dump(x_train, f)\n",
    "    \n",
    "with open ('pickle_files/y_train.pkl', 'wb') as f:\n",
    "    f= pickle.dump(y_train, f)\n",
    "\n",
    "with open ('pickle_files/x_test.pkl', 'wb') as f:\n",
    "    f= pickle.dump(X_test, f)\n",
    "\n",
    "with open ('pickle_files/y_test.pkl', 'wb') as f:\n",
    "    f= pickle.dump(y_test, f)\n",
    "    \n",
    "with open ('pickle_files/x_cv.pkl', 'wb') as f:\n",
    "    f= pickle.dump(x_cv, f)\n",
    "    \n",
    "with open ('pickle_files/y_cv.pkl', 'wb') as f:\n",
    "    f= pickle.dump(y_cv, f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UEpGMYOh8Kyq"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('pickle_files/x_train.pkl', 'rb') as f:\n",
    "    x_train= pickle.load(f)\n",
    "    \n",
    "with open ('pickle_files/y_train.pkl', 'rb') as f:\n",
    "    y_train= pickle.load( f)\n",
    "\n",
    "with open ('pickle_files/x_test.pkl', 'rb') as f:\n",
    "    X_test= pickle.load(f)\n",
    "\n",
    "with open ('pickle_files/y_test.pkl', 'rb') as f:\n",
    "    y_test= pickle.load(f)\n",
    "    \n",
    "with open ('pickle_files/x_cv.pkl', 'rb') as f:\n",
    "    x_cv= pickle.load(f)\n",
    "    \n",
    "with open ('pickle_files/y_cv.pkl', 'rb') as f:\n",
    "    y_cv= pickle.load(f)\n",
    "    \n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RU_d0g8XpBMi"
   },
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    zca_epsilon=1e-06,\n",
    "        rotation_range=15,  #  rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.3, # zoom image \n",
    "        width_shift_range=0.20,  #  shift width\n",
    "        height_shift_range=0.20,  # shift height\n",
    "        horizontal_flip=True, rescale=1./255,fill_mode='nearest')  # flip images\n",
    "\n",
    "\n",
    "\n",
    "train_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0thkMExL9-5X",
    "outputId": "8857a0ef-6a53-47d7-8f7a-ce4ea386af7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_height, img_width, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnIZRYIuczip"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ki_A59V72kk6"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "#In a DenseNet we have a dense block where we have Batch Norm>Relu>Conv>DropOut and all the features are concatenated at each convolution.\n",
    "\n",
    "def denseblock(input, num_filter, dropout_rate):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter, dropout_rate):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3R4s4OT-LK0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_layers=40\n",
    "l = (total_layers-4)/3\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kx3MN7ZOFyHs"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128 #As per the research Paper\n",
    "num_classes = 10\n",
    "epochs = 300\n",
    "total_layers=40\n",
    "l = 18\n",
    "num_filter = 14\n",
    "compression = 0.9\n",
    "dropout_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8B8b-XyrkYP",
    "outputId": "7b86cd39-eeb1-476b-d5ca-684f9b922a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 14)   378         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 14)  56          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 14)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 12)   1512        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 26)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 26)  104         ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 26)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 12)   2808        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 38)   0           ['concatenate[0][0]',            \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 38)  152         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 38)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 12)   4104        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 50)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 50)  200         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 50)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 12)   5400        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 62)   0           ['concatenate_2[0][0]',          \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 62)  248         ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 62)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 12)   6696        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 74)   0           ['concatenate_3[0][0]',          \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 74)  296         ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 74)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 12)   7992        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 86)   0           ['concatenate_4[0][0]',          \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 86)  344         ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 86)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 12)   9288        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 98)   0           ['concatenate_5[0][0]',          \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 98)  392         ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 98)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 12)   10584       ['activation_7[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 110)  0           ['concatenate_6[0][0]',          \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 110)  440        ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 110)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 12)   11880       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 122)  0           ['concatenate_7[0][0]',          \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 122)  488        ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 122)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 12)   13176       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 134)  0           ['concatenate_8[0][0]',          \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 134)  536        ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 134)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 12)   14472       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 146)  0           ['concatenate_9[0][0]',          \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 146)  584        ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 146)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 12)   15768       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 158)  0           ['concatenate_10[0][0]',         \n",
      "                                                                  'conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 158)  632        ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 158)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 12)   17064       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 32, 32, 170)  0           ['concatenate_11[0][0]',         \n",
      "                                                                  'conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 170)  680        ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 170)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 12)   18360       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 32, 32, 182)  0           ['concatenate_12[0][0]',         \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 182)  728        ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 182)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 32, 32, 12)   19656       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 32, 32, 194)  0           ['concatenate_13[0][0]',         \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 194)  776        ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 32, 32, 194)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 32, 32, 12)   20952       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 32, 32, 206)  0           ['concatenate_14[0][0]',         \n",
      "                                                                  'conv2d_16[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 32, 32, 206)  824        ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 32, 32, 206)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 32, 32, 12)   22248       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 32, 32, 218)  0           ['concatenate_15[0][0]',         \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 218)  872        ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 32, 32, 218)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 12)   23544       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 32, 32, 230)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 230)  920        ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 230)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 32, 32, 12)   2760        ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 12)  0           ['conv2d_19[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 12)  48          ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 12)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 12)   1296        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 24)   0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 24)  96          ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 24)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 12)   2592        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 16, 16, 36)   0           ['concatenate_18[0][0]',         \n",
      "                                                                  'conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 36)  144         ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 36)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 12)   3888        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 16, 16, 48)   0           ['concatenate_19[0][0]',         \n",
      "                                                                  'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 48)  192         ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 48)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 12)   5184        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 16, 16, 60)   0           ['concatenate_20[0][0]',         \n",
      "                                                                  'conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 60)  240         ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 60)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 12)   6480        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_21[0][0]',         \n",
      "                                                                  'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 72)  288         ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 12)   7776        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 84)   0           ['concatenate_22[0][0]',         \n",
      "                                                                  'conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 84)  336         ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 84)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 12)   9072        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 16, 16, 96)   0           ['concatenate_23[0][0]',         \n",
      "                                                                  'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 96)  384         ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 12)   10368       ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 16, 16, 108)  0           ['concatenate_24[0][0]',         \n",
      "                                                                  'conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 108)  432        ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 108)  0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 12)   11664       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 16, 16, 120)  0           ['concatenate_25[0][0]',         \n",
      "                                                                  'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 120)  480        ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 120)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 12)   12960       ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 16, 16, 132)  0           ['concatenate_26[0][0]',         \n",
      "                                                                  'conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 132)  528        ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 132)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 12)   14256       ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 16, 16, 144)  0           ['concatenate_27[0][0]',         \n",
      "                                                                  'conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 144)  576        ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 144)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 12)   15552       ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 16, 16, 156)  0           ['concatenate_28[0][0]',         \n",
      "                                                                  'conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 156)  624        ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 156)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 12)   16848       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 16, 16, 168)  0           ['concatenate_29[0][0]',         \n",
      "                                                                  'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16, 16, 168)  672        ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 16, 16, 168)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_33 (Conv2D)             (None, 16, 16, 12)   18144       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 16, 16, 180)  0           ['concatenate_30[0][0]',         \n",
      "                                                                  'conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 16, 16, 180)  720        ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 16, 16, 180)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 12)   19440       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 16, 16, 192)  0           ['concatenate_31[0][0]',         \n",
      "                                                                  'conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 16, 16, 192)  768        ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 16, 16, 192)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 16, 16, 12)   20736       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 16, 16, 204)  0           ['concatenate_32[0][0]',         \n",
      "                                                                  'conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 16, 16, 204)  816        ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 16, 16, 204)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 16, 16, 12)   22032       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 16, 16, 216)  0           ['concatenate_33[0][0]',         \n",
      "                                                                  'conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16, 16, 216)  864        ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 16, 16, 216)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 16, 16, 12)   23328       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 16, 16, 228)  0           ['concatenate_34[0][0]',         \n",
      "                                                                  'conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16, 16, 228)  912        ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 16, 16, 228)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 16, 16, 12)   2736        ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 12)    0           ['conv2d_38[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 12)    48          ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 12)     0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 12)     1296        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 8, 8, 24)     0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 8, 8, 24)    96          ['concatenate_36[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 8, 8, 24)     0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 12)     2592        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 8, 8, 36)     0           ['concatenate_36[0][0]',         \n",
      "                                                                  'conv2d_40[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 8, 8, 36)    144         ['concatenate_37[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 8, 8, 36)     0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 12)     3888        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenate)   (None, 8, 8, 48)     0           ['concatenate_37[0][0]',         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 8, 8, 48)    192         ['concatenate_38[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 8, 8, 48)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 12)     5184        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenate)   (None, 8, 8, 60)     0           ['concatenate_38[0][0]',         \n",
      "                                                                  'conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 8, 8, 60)    240         ['concatenate_39[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 8, 8, 60)     0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 12)     6480        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 8, 8, 72)     0           ['concatenate_39[0][0]',         \n",
      "                                                                  'conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 8, 8, 72)    288         ['concatenate_40[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 8, 8, 72)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 12)     7776        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 8, 8, 84)     0           ['concatenate_40[0][0]',         \n",
      "                                                                  'conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 8, 8, 84)    336         ['concatenate_41[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 8, 8, 84)     0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 8, 8, 12)     9072        ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 8, 8, 96)     0           ['concatenate_41[0][0]',         \n",
      "                                                                  'conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 8, 8, 96)    384         ['concatenate_42[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 8, 8, 96)     0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 8, 8, 12)     10368       ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 8, 8, 108)    0           ['concatenate_42[0][0]',         \n",
      "                                                                  'conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 8, 8, 108)   432         ['concatenate_43[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 8, 8, 108)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 8, 8, 12)     11664       ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 8, 8, 120)    0           ['concatenate_43[0][0]',         \n",
      "                                                                  'conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 8, 8, 120)   480         ['concatenate_44[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 8, 8, 120)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 8, 8, 12)     12960       ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 8, 8, 132)    0           ['concatenate_44[0][0]',         \n",
      "                                                                  'conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 8, 8, 132)   528         ['concatenate_45[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 8, 8, 132)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 8, 8, 12)     14256       ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 8, 8, 144)    0           ['concatenate_45[0][0]',         \n",
      "                                                                  'conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 8, 8, 144)   576         ['concatenate_46[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 8, 8, 144)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 8, 8, 12)     15552       ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 8, 8, 156)    0           ['concatenate_46[0][0]',         \n",
      "                                                                  'conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 8, 8, 156)   624         ['concatenate_47[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 8, 8, 156)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 8, 8, 12)     16848       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_48 (Concatenate)   (None, 8, 8, 168)    0           ['concatenate_47[0][0]',         \n",
      "                                                                  'conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 168)   672         ['concatenate_48[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 168)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 8, 8, 12)     18144       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenate)   (None, 8, 8, 180)    0           ['concatenate_48[0][0]',         \n",
      "                                                                  'conv2d_52[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 180)   720         ['concatenate_49[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 180)    0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 8, 8, 12)     19440       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_50 (Concatenate)   (None, 8, 8, 192)    0           ['concatenate_49[0][0]',         \n",
      "                                                                  'conv2d_53[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 192)   768         ['concatenate_50[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 8, 8, 12)     20736       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_51 (Concatenate)   (None, 8, 8, 204)    0           ['concatenate_50[0][0]',         \n",
      "                                                                  'conv2d_54[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 204)   816         ['concatenate_51[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 204)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 8, 8, 12)     22032       ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_52 (Concatenate)   (None, 8, 8, 216)    0           ['concatenate_51[0][0]',         \n",
      "                                                                  'conv2d_55[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 216)   864         ['concatenate_52[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 216)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 12)     23328       ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_53 (Concatenate)   (None, 8, 8, 228)    0           ['concatenate_52[0][0]',         \n",
      "                                                                  'conv2d_56[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 228)   912         ['concatenate_53[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 228)    0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 8, 8, 12)     2736        ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 4, 4, 12)    0           ['conv2d_57[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 4, 4, 12)    48          ['average_pooling2d_2[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 4, 4, 12)     0           ['batch_normalization_57[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 4, 4, 12)     1296        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_54 (Concatenate)   (None, 4, 4, 24)     0           ['average_pooling2d_2[0][0]',    \n",
      "                                                                  'conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 4, 4, 24)    96          ['concatenate_54[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 4, 4, 24)     0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 4, 4, 12)     2592        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_55 (Concatenate)   (None, 4, 4, 36)     0           ['concatenate_54[0][0]',         \n",
      "                                                                  'conv2d_59[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 4, 4, 36)    144         ['concatenate_55[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 4, 4, 36)     0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 4, 4, 12)     3888        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_56 (Concatenate)   (None, 4, 4, 48)     0           ['concatenate_55[0][0]',         \n",
      "                                                                  'conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 4, 4, 48)    192         ['concatenate_56[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 4, 4, 48)     0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 4, 4, 12)     5184        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_57 (Concatenate)   (None, 4, 4, 60)     0           ['concatenate_56[0][0]',         \n",
      "                                                                  'conv2d_61[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 4, 4, 60)    240         ['concatenate_57[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 4, 4, 60)     0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 4, 4, 12)     6480        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_58 (Concatenate)   (None, 4, 4, 72)     0           ['concatenate_57[0][0]',         \n",
      "                                                                  'conv2d_62[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 4, 4, 72)    288         ['concatenate_58[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 4, 4, 72)     0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 4, 4, 12)     7776        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_59 (Concatenate)   (None, 4, 4, 84)     0           ['concatenate_58[0][0]',         \n",
      "                                                                  'conv2d_63[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 4, 4, 84)    336         ['concatenate_59[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 4, 4, 84)     0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 4, 4, 12)     9072        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_60 (Concatenate)   (None, 4, 4, 96)     0           ['concatenate_59[0][0]',         \n",
      "                                                                  'conv2d_64[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 4, 4, 96)    384         ['concatenate_60[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 4, 4, 96)     0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 4, 4, 12)     10368       ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_61 (Concatenate)   (None, 4, 4, 108)    0           ['concatenate_60[0][0]',         \n",
      "                                                                  'conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 4, 4, 108)   432         ['concatenate_61[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 4, 4, 108)    0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 4, 4, 12)     11664       ['activation_65[0][0]']          \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concatenate_62 (Concatenate)   (None, 4, 4, 120)    0           ['concatenate_61[0][0]',         \n",
      "                                                                  'conv2d_66[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 4, 4, 120)   480         ['concatenate_62[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 4, 4, 120)    0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 4, 4, 12)     12960       ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_63 (Concatenate)   (None, 4, 4, 132)    0           ['concatenate_62[0][0]',         \n",
      "                                                                  'conv2d_67[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 4, 4, 132)   528         ['concatenate_63[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 4, 4, 132)    0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 4, 4, 12)     14256       ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_64 (Concatenate)   (None, 4, 4, 144)    0           ['concatenate_63[0][0]',         \n",
      "                                                                  'conv2d_68[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 4, 4, 144)   576         ['concatenate_64[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 4, 4, 144)    0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 4, 4, 12)     15552       ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_65 (Concatenate)   (None, 4, 4, 156)    0           ['concatenate_64[0][0]',         \n",
      "                                                                  'conv2d_69[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 4, 4, 156)   624         ['concatenate_65[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 4, 4, 156)    0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 4, 4, 12)     16848       ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_66 (Concatenate)   (None, 4, 4, 168)    0           ['concatenate_65[0][0]',         \n",
      "                                                                  'conv2d_70[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 4, 4, 168)   672         ['concatenate_66[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 4, 4, 168)    0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 4, 4, 12)     18144       ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_67 (Concatenate)   (None, 4, 4, 180)    0           ['concatenate_66[0][0]',         \n",
      "                                                                  'conv2d_71[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 4, 4, 180)   720         ['concatenate_67[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 4, 4, 180)    0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 4, 4, 12)     19440       ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_68 (Concatenate)   (None, 4, 4, 192)    0           ['concatenate_67[0][0]',         \n",
      "                                                                  'conv2d_72[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 4, 4, 192)   768         ['concatenate_68[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 4, 4, 192)    0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 4, 4, 12)     20736       ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_69 (Concatenate)   (None, 4, 4, 204)    0           ['concatenate_68[0][0]',         \n",
      "                                                                  'conv2d_73[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 4, 4, 204)   816         ['concatenate_69[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 4, 4, 204)    0           ['batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 4, 4, 12)     22032       ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_70 (Concatenate)   (None, 4, 4, 216)    0           ['concatenate_69[0][0]',         \n",
      "                                                                  'conv2d_74[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_74 (BatchN  (None, 4, 4, 216)   864         ['concatenate_70[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 4, 4, 216)    0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 4, 4, 12)     23328       ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_71 (Concatenate)   (None, 4, 4, 228)    0           ['concatenate_70[0][0]',         \n",
      "                                                                  'conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 4, 4, 228)   912         ['concatenate_71[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 4, 4, 228)    0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 2, 2, 228)   0           ['activation_75[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 912)          0           ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           9130        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 944,724\n",
      "Trainable params: 926,408\n",
      "Non-trainable params: 18,316\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same',kernel_initializer='he_uniform',kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4))(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)\n",
    "\n",
    "model=Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w1SVVJaGWjqT"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/65891168/keras-modelcheckpoint-not-saving-but-earlystopping-is-working-fine-with-the-same\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,x_val,y_val):\n",
    "        super().__init__()\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        pass\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': []}\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['accuracy'].append(logs.get('accuracy'))\n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_accuracy', -1) != -1:\n",
    "            self.history['val_accuracy'].append(logs.get('val_accuracy'))\n",
    "            \n",
    "        y_pred=tf.reshape((self.model.predict(self.x_val)),(1,-1)).numpy()[0]\n",
    "        y_pred[y_pred==-1]=0\n",
    "        y_pred=y_pred.astype('int32')\n",
    "        y_true=self.y_val.astype('int32')\n",
    "        \n",
    "        file_path=\"saved_models/weights_\"+'epoch_'+str(epoch)+'.hdf5' \n",
    "        \n",
    "        if epoch>2:\n",
    "            if self.history['val_accuracy'][-1]>=0.90:\n",
    "                self.model.save(file_path)\n",
    "                if all([self.history['val_accuracy'][-1]<=self.history['val_accuracy'][-2],self.history['val_accuracy'][-1]<=self.history['val_accuracy'][-3],self.history['val_accuracy'][-2]<=self.history['val_accuracy'][-3]]):\n",
    "                    print('val_accuracy not improving since the last 2 epochs, hence stopping training')\n",
    "                    self.model.stop_training=True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-XwuEi1QrVk_"
   },
   "outputs": [],
   "source": [
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=5, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.0001)\n",
    "\n",
    "paths='saved_models/final_models/f_model_2.hdf5'\n",
    "\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(paths, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eL4AhJwDAwFn",
    "outputId": "d933b71c-eea6-4ff9-b72b-07be5cc6e394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.0412 - accuracy: 0.2729\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27430, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 19ms/step\n",
      "313/313 [==============================] - 159s 410ms/step - loss: 2.0412 - accuracy: 0.2729 - val_loss: 2.0479 - val_accuracy: 0.2743 - lr: 0.1000\n",
      "Epoch 2/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.7306 - accuracy: 0.3692\n",
      "Epoch 2: val_accuracy improved from 0.27430 to 0.31030, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 122s 391ms/step - loss: 1.7306 - accuracy: 0.3692 - val_loss: 2.0227 - val_accuracy: 0.3103 - lr: 0.1000\n",
      "Epoch 3/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.5958 - accuracy: 0.4179\n",
      "Epoch 3: val_accuracy did not improve from 0.31030\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 122s 389ms/step - loss: 1.5958 - accuracy: 0.4179 - val_loss: 2.3367 - val_accuracy: 0.2949 - lr: 0.1000\n",
      "Epoch 4/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4820 - accuracy: 0.4627\n",
      "Epoch 4: val_accuracy improved from 0.31030 to 0.46840, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 126s 401ms/step - loss: 1.4820 - accuracy: 0.4627 - val_loss: 1.5496 - val_accuracy: 0.4684 - lr: 0.1000\n",
      "Epoch 5/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3908 - accuracy: 0.4996\n",
      "Epoch 5: val_accuracy did not improve from 0.46840\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 121s 387ms/step - loss: 1.3908 - accuracy: 0.4996 - val_loss: 2.2493 - val_accuracy: 0.3454 - lr: 0.1000\n",
      "Epoch 6/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3054 - accuracy: 0.5345\n",
      "Epoch 6: val_accuracy did not improve from 0.46840\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 122s 390ms/step - loss: 1.3054 - accuracy: 0.5345 - val_loss: 2.1620 - val_accuracy: 0.3654 - lr: 0.1000\n",
      "Epoch 7/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2279 - accuracy: 0.5624\n",
      "Epoch 7: val_accuracy improved from 0.46840 to 0.55980, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 24ms/step\n",
      "313/313 [==============================] - 121s 387ms/step - loss: 1.2279 - accuracy: 0.5624 - val_loss: 1.3165 - val_accuracy: 0.5598 - lr: 0.1000\n",
      "Epoch 8/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.1554 - accuracy: 0.5892\n",
      "Epoch 8: val_accuracy did not improve from 0.55980\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 120s 384ms/step - loss: 1.1554 - accuracy: 0.5892 - val_loss: 1.3121 - val_accuracy: 0.5477 - lr: 0.1000\n",
      "Epoch 9/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.0964 - accuracy: 0.6102\n",
      "Epoch 9: val_accuracy did not improve from 0.55980\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 120s 383ms/step - loss: 1.0964 - accuracy: 0.6102 - val_loss: 2.0605 - val_accuracy: 0.4777 - lr: 0.1000\n",
      "Epoch 10/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.0453 - accuracy: 0.6326\n",
      "Epoch 10: val_accuracy improved from 0.55980 to 0.62370, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "313/313 [==============================] - 119s 380ms/step - loss: 1.0453 - accuracy: 0.6326 - val_loss: 1.1308 - val_accuracy: 0.6237 - lr: 0.1000\n",
      "Epoch 11/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.9792 - accuracy: 0.6535\n",
      "Epoch 11: val_accuracy did not improve from 0.62370\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 118s 377ms/step - loss: 0.9792 - accuracy: 0.6535 - val_loss: 1.5866 - val_accuracy: 0.5465 - lr: 0.1000\n",
      "Epoch 12/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.9278 - accuracy: 0.6740\n",
      "Epoch 12: val_accuracy did not improve from 0.62370\n",
      "313/313 [==============================] - 7s 24ms/step\n",
      "313/313 [==============================] - 118s 378ms/step - loss: 0.9278 - accuracy: 0.6740 - val_loss: 1.8451 - val_accuracy: 0.5213 - lr: 0.1000\n",
      "Epoch 13/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.8892 - accuracy: 0.6894\n",
      "Epoch 13: val_accuracy improved from 0.62370 to 0.62850, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 120s 382ms/step - loss: 0.8892 - accuracy: 0.6894 - val_loss: 1.2806 - val_accuracy: 0.6285 - lr: 0.1000\n",
      "Epoch 14/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.8481 - accuracy: 0.7032\n",
      "Epoch 14: val_accuracy improved from 0.62850 to 0.65030, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 119s 379ms/step - loss: 0.8481 - accuracy: 0.7032 - val_loss: 1.0956 - val_accuracy: 0.6503 - lr: 0.1000\n",
      "Epoch 15/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.8169 - accuracy: 0.7159\n",
      "Epoch 15: val_accuracy improved from 0.65030 to 0.68050, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 121s 385ms/step - loss: 0.8169 - accuracy: 0.7159 - val_loss: 1.0236 - val_accuracy: 0.6805 - lr: 0.1000\n",
      "Epoch 16/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.7855 - accuracy: 0.7258\n",
      "Epoch 16: val_accuracy did not improve from 0.68050\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 123s 393ms/step - loss: 0.7855 - accuracy: 0.7258 - val_loss: 1.0422 - val_accuracy: 0.6640 - lr: 0.1000\n",
      "Epoch 17/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.7520 - accuracy: 0.7368\n",
      "Epoch 17: val_accuracy improved from 0.68050 to 0.70350, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 122s 391ms/step - loss: 0.7520 - accuracy: 0.7368 - val_loss: 0.9280 - val_accuracy: 0.7035 - lr: 0.1000\n",
      "Epoch 18/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.7274 - accuracy: 0.7477\n",
      "Epoch 18: val_accuracy improved from 0.70350 to 0.73560, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 119s 381ms/step - loss: 0.7274 - accuracy: 0.7477 - val_loss: 0.7678 - val_accuracy: 0.7356 - lr: 0.1000\n",
      "Epoch 19/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.6977 - accuracy: 0.7602\n",
      "Epoch 19: val_accuracy did not improve from 0.73560\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 119s 379ms/step - loss: 0.6977 - accuracy: 0.7602 - val_loss: 0.9418 - val_accuracy: 0.7049 - lr: 0.1000\n",
      "Epoch 20/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.6836 - accuracy: 0.7623\n",
      "Epoch 20: val_accuracy improved from 0.73560 to 0.73690, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.6836 - accuracy: 0.7623 - val_loss: 0.8036 - val_accuracy: 0.7369 - lr: 0.1000\n",
      "Epoch 21/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.7717\n",
      "Epoch 21: val_accuracy improved from 0.73690 to 0.74400, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 122s 390ms/step - loss: 0.6617 - accuracy: 0.7717 - val_loss: 0.8117 - val_accuracy: 0.7440 - lr: 0.1000\n",
      "Epoch 22/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.7780\n",
      "Epoch 22: val_accuracy improved from 0.74400 to 0.76660, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 123s 392ms/step - loss: 0.6431 - accuracy: 0.7780 - val_loss: 0.7173 - val_accuracy: 0.7666 - lr: 0.1000\n",
      "Epoch 23/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.7842\n",
      "Epoch 23: val_accuracy improved from 0.76660 to 0.78050, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 120s 384ms/step - loss: 0.6265 - accuracy: 0.7842 - val_loss: 0.6838 - val_accuracy: 0.7805 - lr: 0.1000\n",
      "Epoch 24/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.7908\n",
      "Epoch 24: val_accuracy did not improve from 0.78050\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 124s 398ms/step - loss: 0.6079 - accuracy: 0.7908 - val_loss: 0.6888 - val_accuracy: 0.7785 - lr: 0.1000\n",
      "Epoch 25/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.6047 - accuracy: 0.7932\n",
      "Epoch 25: val_accuracy improved from 0.78050 to 0.81060, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 125s 399ms/step - loss: 0.6047 - accuracy: 0.7932 - val_loss: 0.5791 - val_accuracy: 0.8106 - lr: 0.1000\n",
      "Epoch 26/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.7975\n",
      "Epoch 26: val_accuracy did not improve from 0.81060\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 125s 399ms/step - loss: 0.5859 - accuracy: 0.7975 - val_loss: 0.7399 - val_accuracy: 0.7672 - lr: 0.1000\n",
      "Epoch 27/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.8047\n",
      "Epoch 27: val_accuracy did not improve from 0.81060\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 117s 374ms/step - loss: 0.5716 - accuracy: 0.8047 - val_loss: 0.6498 - val_accuracy: 0.7893 - lr: 0.1000\n",
      "Epoch 28/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.8071\n",
      "Epoch 28: val_accuracy did not improve from 0.81060\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 120s 385ms/step - loss: 0.5588 - accuracy: 0.8071 - val_loss: 0.6542 - val_accuracy: 0.7894 - lr: 0.1000\n",
      "Epoch 29/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.5471 - accuracy: 0.8123\n",
      "Epoch 29: val_accuracy did not improve from 0.81060\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 121s 386ms/step - loss: 0.5471 - accuracy: 0.8123 - val_loss: 0.9262 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 30/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.8137\n",
      "Epoch 30: val_accuracy did not improve from 0.81060\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 388ms/step - loss: 0.5424 - accuracy: 0.8137 - val_loss: 0.8237 - val_accuracy: 0.7428 - lr: 0.1000\n",
      "Epoch 31/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.8329\n",
      "Epoch 31: val_accuracy improved from 0.81060 to 0.82520, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 386ms/step - loss: 0.4858 - accuracy: 0.8329 - val_loss: 0.5205 - val_accuracy: 0.8252 - lr: 0.0500\n",
      "Epoch 32/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.8386\n",
      "Epoch 32: val_accuracy did not improve from 0.82520\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 125s 400ms/step - loss: 0.4723 - accuracy: 0.8386 - val_loss: 0.6123 - val_accuracy: 0.8104 - lr: 0.0500\n",
      "Epoch 33/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.8421\n",
      "Epoch 33: val_accuracy did not improve from 0.82520\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 125s 398ms/step - loss: 0.4644 - accuracy: 0.8421 - val_loss: 0.5927 - val_accuracy: 0.8143 - lr: 0.0500\n",
      "Epoch 34/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4554 - accuracy: 0.8425\n",
      "Epoch 34: val_accuracy did not improve from 0.82520\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 125s 399ms/step - loss: 0.4554 - accuracy: 0.8425 - val_loss: 0.5483 - val_accuracy: 0.8241 - lr: 0.0500\n",
      "Epoch 35/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4467 - accuracy: 0.8478\n",
      "Epoch 35: val_accuracy improved from 0.82520 to 0.83260, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 121s 388ms/step - loss: 0.4467 - accuracy: 0.8478 - val_loss: 0.5178 - val_accuracy: 0.8326 - lr: 0.0500\n",
      "Epoch 36/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4473 - accuracy: 0.8464\n",
      "Epoch 36: val_accuracy did not improve from 0.83260\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 120s 384ms/step - loss: 0.4473 - accuracy: 0.8464 - val_loss: 0.5939 - val_accuracy: 0.8153 - lr: 0.0500\n",
      "Epoch 37/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.8481\n",
      "Epoch 37: val_accuracy did not improve from 0.83260\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 122s 390ms/step - loss: 0.4397 - accuracy: 0.8481 - val_loss: 0.5805 - val_accuracy: 0.8232 - lr: 0.0500\n",
      "Epoch 38/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8531\n",
      "Epoch 38: val_accuracy improved from 0.83260 to 0.84360, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 395ms/step - loss: 0.4287 - accuracy: 0.8531 - val_loss: 0.4906 - val_accuracy: 0.8436 - lr: 0.0500\n",
      "Epoch 39/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4254 - accuracy: 0.8540\n",
      "Epoch 39: val_accuracy did not improve from 0.84360\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "313/313 [==============================] - 117s 375ms/step - loss: 0.4254 - accuracy: 0.8540 - val_loss: 0.5258 - val_accuracy: 0.8346 - lr: 0.0500\n",
      "Epoch 40/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8545\n",
      "Epoch 40: val_accuracy improved from 0.84360 to 0.85320, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 119s 382ms/step - loss: 0.4241 - accuracy: 0.8545 - val_loss: 0.4510 - val_accuracy: 0.8532 - lr: 0.0500\n",
      "Epoch 41/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8552\n",
      "Epoch 41: val_accuracy did not improve from 0.85320\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 118s 376ms/step - loss: 0.4199 - accuracy: 0.8552 - val_loss: 0.5832 - val_accuracy: 0.8203 - lr: 0.0500\n",
      "Epoch 42/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.8546\n",
      "Epoch 42: val_accuracy did not improve from 0.85320\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 116s 371ms/step - loss: 0.4200 - accuracy: 0.8546 - val_loss: 0.7397 - val_accuracy: 0.7867 - lr: 0.0500\n",
      "Epoch 43/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4135 - accuracy: 0.8570\n",
      "Epoch 43: val_accuracy did not improve from 0.85320\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 386ms/step - loss: 0.4135 - accuracy: 0.8570 - val_loss: 0.6343 - val_accuracy: 0.8053 - lr: 0.0500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.8588\n",
      "Epoch 44: val_accuracy did not improve from 0.85320\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 125s 401ms/step - loss: 0.4085 - accuracy: 0.8588 - val_loss: 0.4537 - val_accuracy: 0.8512 - lr: 0.0500\n",
      "Epoch 45/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4057 - accuracy: 0.8619\n",
      "Epoch 45: val_accuracy did not improve from 0.85320\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 118s 375ms/step - loss: 0.4057 - accuracy: 0.8619 - val_loss: 0.7965 - val_accuracy: 0.7714 - lr: 0.0500\n",
      "Epoch 46/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3711 - accuracy: 0.8733\n",
      "Epoch 46: val_accuracy improved from 0.85320 to 0.86190, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 125s 401ms/step - loss: 0.3711 - accuracy: 0.8733 - val_loss: 0.4291 - val_accuracy: 0.8619 - lr: 0.0250\n",
      "Epoch 47/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3626 - accuracy: 0.8756\n",
      "Epoch 47: val_accuracy did not improve from 0.86190\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 387ms/step - loss: 0.3626 - accuracy: 0.8756 - val_loss: 0.4949 - val_accuracy: 0.8491 - lr: 0.0250\n",
      "Epoch 48/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.8771\n",
      "Epoch 48: val_accuracy improved from 0.86190 to 0.86290, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 118s 376ms/step - loss: 0.3554 - accuracy: 0.8771 - val_loss: 0.4236 - val_accuracy: 0.8629 - lr: 0.0250\n",
      "Epoch 49/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8760\n",
      "Epoch 49: val_accuracy improved from 0.86290 to 0.86660, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 122s 389ms/step - loss: 0.3564 - accuracy: 0.8760 - val_loss: 0.4284 - val_accuracy: 0.8666 - lr: 0.0250\n",
      "Epoch 50/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.8794\n",
      "Epoch 50: val_accuracy improved from 0.86660 to 0.87050, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 387ms/step - loss: 0.3496 - accuracy: 0.8794 - val_loss: 0.4151 - val_accuracy: 0.8705 - lr: 0.0250\n",
      "Epoch 51/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8785\n",
      "Epoch 51: val_accuracy did not improve from 0.87050\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 117s 374ms/step - loss: 0.3478 - accuracy: 0.8785 - val_loss: 0.4307 - val_accuracy: 0.8647 - lr: 0.0250\n",
      "Epoch 52/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8818\n",
      "Epoch 52: val_accuracy did not improve from 0.87050\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 120s 384ms/step - loss: 0.3444 - accuracy: 0.8818 - val_loss: 0.4491 - val_accuracy: 0.8596 - lr: 0.0250\n",
      "Epoch 53/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8813\n",
      "Epoch 53: val_accuracy did not improve from 0.87050\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 388ms/step - loss: 0.3417 - accuracy: 0.8813 - val_loss: 0.4724 - val_accuracy: 0.8527 - lr: 0.0250\n",
      "Epoch 54/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.8819\n",
      "Epoch 54: val_accuracy did not improve from 0.87050\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 125s 399ms/step - loss: 0.3391 - accuracy: 0.8819 - val_loss: 0.4178 - val_accuracy: 0.8660 - lr: 0.0250\n",
      "Epoch 55/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.8833\n",
      "Epoch 55: val_accuracy did not improve from 0.87050\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "313/313 [==============================] - 7s 24ms/step\n",
      "313/313 [==============================] - 128s 410ms/step - loss: 0.3409 - accuracy: 0.8833 - val_loss: 0.4610 - val_accuracy: 0.8558 - lr: 0.0250\n",
      "Epoch 56/150\n"
     ]
    }
   ],
   "source": [
    "nadam = tf.keras.optimizers.legacy.Nadam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=1e-4)\n",
    "\n",
    "model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "logs='logs/model_1'\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=logs)\n",
    "\n",
    "model.fit(train_datagen.flow(x_train, y_train,batch_size=128),epochs=150, validation_data=(x_cv, y_cv), callbacks=[tensorboard,checkpoint_save,learning_rate_reduction,CustomCallback(x_cv, y_cv)], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('saved_models/final_models/f_model_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8870\n",
      "Epoch 1: val_accuracy improved from -inf to 0.86640, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 19ms/step\n",
      "313/313 [==============================] - 160s 412ms/step - loss: 0.3304 - accuracy: 0.8870 - val_loss: 0.4409 - val_accuracy: 0.8664 - lr: 0.0125\n",
      "Epoch 2/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3263 - accuracy: 0.8884\n",
      "Epoch 2: val_accuracy improved from 0.86640 to 0.87450, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 125s 398ms/step - loss: 0.3263 - accuracy: 0.8884 - val_loss: 0.3911 - val_accuracy: 0.8745 - lr: 0.0125\n",
      "Epoch 3/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8900\n",
      "Epoch 3: val_accuracy improved from 0.87450 to 0.88020, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.3229 - accuracy: 0.8900 - val_loss: 0.3772 - val_accuracy: 0.8802 - lr: 0.0125\n",
      "Epoch 4/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.8895\n",
      "Epoch 4: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 123s 394ms/step - loss: 0.3236 - accuracy: 0.8895 - val_loss: 0.4263 - val_accuracy: 0.8677 - lr: 0.0125\n",
      "Epoch 5/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8893\n",
      "Epoch 5: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 127s 407ms/step - loss: 0.3234 - accuracy: 0.8893 - val_loss: 0.4165 - val_accuracy: 0.8676 - lr: 0.0125\n",
      "Epoch 6/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.8902\n",
      "Epoch 6: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 126s 404ms/step - loss: 0.3184 - accuracy: 0.8902 - val_loss: 0.4206 - val_accuracy: 0.8674 - lr: 0.0125\n",
      "Epoch 7/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.8910\n",
      "Epoch 7: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 127s 404ms/step - loss: 0.3208 - accuracy: 0.8910 - val_loss: 0.4000 - val_accuracy: 0.8760 - lr: 0.0125\n",
      "Epoch 8/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3131 - accuracy: 0.8920\n",
      "Epoch 8: val_accuracy did not improve from 0.88020\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 122s 391ms/step - loss: 0.3131 - accuracy: 0.8920 - val_loss: 0.4306 - val_accuracy: 0.8676 - lr: 0.0125\n",
      "Epoch 9/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.8954\n",
      "Epoch 9: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 118s 377ms/step - loss: 0.3070 - accuracy: 0.8954 - val_loss: 0.3874 - val_accuracy: 0.8781 - lr: 0.0063\n",
      "Epoch 10/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8971\n",
      "Epoch 10: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 121s 385ms/step - loss: 0.3026 - accuracy: 0.8971 - val_loss: 0.3968 - val_accuracy: 0.8769 - lr: 0.0063\n",
      "Epoch 11/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.8999\n",
      "Epoch 11: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 7s 24ms/step\n",
      "313/313 [==============================] - 119s 379ms/step - loss: 0.2960 - accuracy: 0.8999 - val_loss: 0.3838 - val_accuracy: 0.8802 - lr: 0.0063\n",
      "Epoch 12/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.8981\n",
      "Epoch 12: val_accuracy did not improve from 0.88020\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 121s 386ms/step - loss: 0.2974 - accuracy: 0.8981 - val_loss: 0.3932 - val_accuracy: 0.8765 - lr: 0.0063\n",
      "Epoch 13/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.8978\n",
      "Epoch 13: val_accuracy did not improve from 0.88020\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 120s 384ms/step - loss: 0.2993 - accuracy: 0.8978 - val_loss: 0.3945 - val_accuracy: 0.8793 - lr: 0.0063\n",
      "Epoch 14/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9009\n",
      "Epoch 14: val_accuracy improved from 0.88020 to 0.88460, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "313/313 [==============================] - 125s 398ms/step - loss: 0.2895 - accuracy: 0.9009 - val_loss: 0.3760 - val_accuracy: 0.8846 - lr: 0.0031\n",
      "Epoch 15/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.9009\n",
      "Epoch 15: val_accuracy did not improve from 0.88460\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.2934 - accuracy: 0.9009 - val_loss: 0.3817 - val_accuracy: 0.8808 - lr: 0.0031\n",
      "Epoch 16/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9018\n",
      "Epoch 16: val_accuracy did not improve from 0.88460\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 118s 377ms/step - loss: 0.2871 - accuracy: 0.9018 - val_loss: 0.3767 - val_accuracy: 0.8839 - lr: 0.0031\n",
      "Epoch 17/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9027\n",
      "Epoch 17: val_accuracy improved from 0.88460 to 0.88600, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 123s 393ms/step - loss: 0.2871 - accuracy: 0.9027 - val_loss: 0.3711 - val_accuracy: 0.8860 - lr: 0.0031\n",
      "Epoch 18/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9018\n",
      "Epoch 18: val_accuracy did not improve from 0.88600\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 126s 401ms/step - loss: 0.2858 - accuracy: 0.9018 - val_loss: 0.3788 - val_accuracy: 0.8836 - lr: 0.0031\n",
      "Epoch 19/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9024\n",
      "Epoch 19: val_accuracy did not improve from 0.88600\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 121s 388ms/step - loss: 0.2877 - accuracy: 0.9024 - val_loss: 0.3687 - val_accuracy: 0.8849 - lr: 0.0031\n",
      "Epoch 20/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.9018\n",
      "Epoch 20: val_accuracy did not improve from 0.88600\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 122s 389ms/step - loss: 0.2851 - accuracy: 0.9018 - val_loss: 0.3760 - val_accuracy: 0.8832 - lr: 0.0031\n",
      "Epoch 21/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.8999\n",
      "Epoch 21: val_accuracy did not improve from 0.88600\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 123s 392ms/step - loss: 0.2901 - accuracy: 0.8999 - val_loss: 0.3744 - val_accuracy: 0.8847 - lr: 0.0031\n",
      "Epoch 22/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2828 - accuracy: 0.9033\n",
      "Epoch 22: val_accuracy did not improve from 0.88600\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 121s 388ms/step - loss: 0.2828 - accuracy: 0.9033 - val_loss: 0.3834 - val_accuracy: 0.8827 - lr: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.9054\n",
      "Epoch 23: val_accuracy did not improve from 0.88600\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 126s 404ms/step - loss: 0.2812 - accuracy: 0.9054 - val_loss: 0.3791 - val_accuracy: 0.8823 - lr: 0.0016\n",
      "Epoch 24/150\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9046\n",
      "Epoch 24: val_accuracy improved from 0.88600 to 0.88630, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 123s 393ms/step - loss: 0.2822 - accuracy: 0.9046 - val_loss: 0.3735 - val_accuracy: 0.8863 - lr: 0.0016\n",
      "Epoch 25/150\n",
      " 64/313 [=====>........................] - ETA: 1:30 - loss: 0.2762 - accuracy: 0.9059"
     ]
    }
   ],
   "source": [
    "#Using pickle files to generate x_train & y_train, so that there is no data leakage\n",
    "nadam = tf.keras.optimizers.legacy.Nadam(learning_rate=0.0125, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=1e-4)\n",
    "\n",
    "model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "logs='logs/model_1'\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=logs)\n",
    "\n",
    "model.fit(train_datagen.flow(x_train, y_train,batch_size=128),epochs=150, validation_data=(x_cv, y_cv), callbacks=[tensorboard,checkpoint_save,learning_rate_reduction,CustomCallback(x_cv, y_cv)], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('saved_models/final_models/f_model_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  6/313 [..............................] - ETA: 1:53 - loss: 0.2855 - accuracy: 0.9062WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2543s vs `on_train_batch_end` time: 0.2591s). Check your callbacks.\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9033\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88380, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 20ms/step\n",
      "313/313 [==============================] - 181s 471ms/step - loss: 0.2820 - accuracy: 0.9033 - val_loss: 0.3773 - val_accuracy: 0.8838 - lr: 0.0016\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.9043\n",
      "Epoch 2: val_accuracy did not improve from 0.88380\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 129s 413ms/step - loss: 0.2804 - accuracy: 0.9043 - val_loss: 0.3825 - val_accuracy: 0.8813 - lr: 0.0016\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.9043\n",
      "Epoch 3: val_accuracy did not improve from 0.88380\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 129s 413ms/step - loss: 0.2762 - accuracy: 0.9043 - val_loss: 0.3793 - val_accuracy: 0.8824 - lr: 0.0016\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.9033\n",
      "Epoch 4: val_accuracy improved from 0.88380 to 0.88450, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 129s 412ms/step - loss: 0.2831 - accuracy: 0.9033 - val_loss: 0.3736 - val_accuracy: 0.8845 - lr: 0.0016\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.9031\n",
      "Epoch 5: val_accuracy did not improve from 0.88450\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 128s 409ms/step - loss: 0.2805 - accuracy: 0.9031 - val_loss: 0.3737 - val_accuracy: 0.8843 - lr: 0.0016\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.9031\n",
      "Epoch 6: val_accuracy did not improve from 0.88450\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 125s 398ms/step - loss: 0.2786 - accuracy: 0.9031 - val_loss: 0.3833 - val_accuracy: 0.8839 - lr: 0.0016\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.9062\n",
      "Epoch 7: val_accuracy improved from 0.88450 to 0.88640, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 125s 400ms/step - loss: 0.2742 - accuracy: 0.9062 - val_loss: 0.3680 - val_accuracy: 0.8864 - lr: 0.0016\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9043\n",
      "Epoch 8: val_accuracy did not improve from 0.88640\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 118s 376ms/step - loss: 0.2792 - accuracy: 0.9043 - val_loss: 0.3844 - val_accuracy: 0.8817 - lr: 0.0016\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.9046\n",
      "Epoch 9: val_accuracy did not improve from 0.88640\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 120s 383ms/step - loss: 0.2761 - accuracy: 0.9046 - val_loss: 0.3739 - val_accuracy: 0.8835 - lr: 0.0016\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9075\n",
      "Epoch 10: val_accuracy did not improve from 0.88640\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 125s 398ms/step - loss: 0.2731 - accuracy: 0.9075 - val_loss: 0.3797 - val_accuracy: 0.8826 - lr: 0.0016\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9054\n",
      "Epoch 11: val_accuracy did not improve from 0.88640\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.2771 - accuracy: 0.9054 - val_loss: 0.3844 - val_accuracy: 0.8823 - lr: 0.0016\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9052\n",
      "Epoch 12: val_accuracy did not improve from 0.88640\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0007999999797903001.\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 120s 384ms/step - loss: 0.2751 - accuracy: 0.9052 - val_loss: 0.3758 - val_accuracy: 0.8849 - lr: 0.0016\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.9056\n",
      "Epoch 13: val_accuracy did not improve from 0.88640\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.2690 - accuracy: 0.9056 - val_loss: 0.3706 - val_accuracy: 0.8852 - lr: 8.0000e-04\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2764 - accuracy: 0.9044\n",
      "Epoch 14: val_accuracy improved from 0.88640 to 0.88650, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 125s 399ms/step - loss: 0.2764 - accuracy: 0.9044 - val_loss: 0.3681 - val_accuracy: 0.8865 - lr: 8.0000e-04\n",
      "Epoch 15/50\n",
      " 90/313 [=======>......................] - ETA: 1:17 - loss: 0.2670 - accuracy: 0.9106"
     ]
    }
   ],
   "source": [
    "#Using pickle files to generate x_train & y_train, so that there is no data leakage\n",
    "nadam = tf.keras.optimizers.legacy.Nadam(learning_rate=0.0016, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=1e-4)\n",
    "\n",
    "model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "logs='logs/model_1'\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=logs)\n",
    "\n",
    "model.fit(train_datagen.flow(x_train, y_train,batch_size=128),epochs=50, validation_data=(x_cv, y_cv), callbacks=[tensorboard,checkpoint_save,learning_rate_reduction,CustomCallback(x_cv, y_cv)], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('saved_models/final_models/f_model_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.9054\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88510, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 20ms/step\n",
      "313/313 [==============================] - 178s 424ms/step - loss: 0.2747 - accuracy: 0.9054 - val_loss: 0.3681 - val_accuracy: 0.8851 - lr: 8.0000e-04\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.9054\n",
      "Epoch 2: val_accuracy improved from 0.88510 to 0.88520, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.2758 - accuracy: 0.9054 - val_loss: 0.3690 - val_accuracy: 0.8852 - lr: 8.0000e-04\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.9071\n",
      "Epoch 3: val_accuracy did not improve from 0.88520\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 125s 400ms/step - loss: 0.2694 - accuracy: 0.9071 - val_loss: 0.3697 - val_accuracy: 0.8851 - lr: 8.0000e-04\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.9058\n",
      "Epoch 4: val_accuracy did not improve from 0.88520\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 124s 395ms/step - loss: 0.2744 - accuracy: 0.9058 - val_loss: 0.3798 - val_accuracy: 0.8833 - lr: 8.0000e-04\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.9034\n",
      "Epoch 5: val_accuracy did not improve from 0.88520\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 126s 402ms/step - loss: 0.2773 - accuracy: 0.9034 - val_loss: 0.3700 - val_accuracy: 0.8850 - lr: 8.0000e-04\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9079\n",
      "Epoch 6: val_accuracy improved from 0.88520 to 0.88650, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "313/313 [==============================] - 122s 389ms/step - loss: 0.2712 - accuracy: 0.9079 - val_loss: 0.3690 - val_accuracy: 0.8865 - lr: 8.0000e-04\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.9051\n",
      "Epoch 7: val_accuracy did not improve from 0.88650\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 396ms/step - loss: 0.2760 - accuracy: 0.9051 - val_loss: 0.3711 - val_accuracy: 0.8842 - lr: 8.0000e-04\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.9060\n",
      "Epoch 8: val_accuracy did not improve from 0.88650\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 122s 388ms/step - loss: 0.2760 - accuracy: 0.9060 - val_loss: 0.3704 - val_accuracy: 0.8846 - lr: 8.0000e-04\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.9060\n",
      "Epoch 9: val_accuracy did not improve from 0.88650\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 125s 400ms/step - loss: 0.2724 - accuracy: 0.9060 - val_loss: 0.3713 - val_accuracy: 0.8854 - lr: 8.0000e-04\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9065\n",
      "Epoch 10: val_accuracy did not improve from 0.88650\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 129s 413ms/step - loss: 0.2730 - accuracy: 0.9065 - val_loss: 0.3670 - val_accuracy: 0.8856 - lr: 8.0000e-04\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9054\n",
      "Epoch 11: val_accuracy did not improve from 0.88650\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00039999998989515007.\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 129s 413ms/step - loss: 0.2755 - accuracy: 0.9054 - val_loss: 0.3728 - val_accuracy: 0.8853 - lr: 8.0000e-04\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9081\n",
      "Epoch 12: val_accuracy did not improve from 0.88650\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 122s 388ms/step - loss: 0.2657 - accuracy: 0.9081 - val_loss: 0.3716 - val_accuracy: 0.8849 - lr: 4.0000e-04\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9060\n",
      "Epoch 13: val_accuracy improved from 0.88650 to 0.88670, saving model to saved_models/final_models/f_model_2.hdf5\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 124s 395ms/step - loss: 0.2714 - accuracy: 0.9060 - val_loss: 0.3671 - val_accuracy: 0.8867 - lr: 4.0000e-04\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.9077\n",
      "Epoch 14: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 395ms/step - loss: 0.2700 - accuracy: 0.9077 - val_loss: 0.3695 - val_accuracy: 0.8859 - lr: 4.0000e-04\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.9072\n",
      "Epoch 15: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 129s 411ms/step - loss: 0.2687 - accuracy: 0.9072 - val_loss: 0.3682 - val_accuracy: 0.8864 - lr: 4.0000e-04\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.9067\n",
      "Epoch 16: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 128s 410ms/step - loss: 0.2727 - accuracy: 0.9067 - val_loss: 0.3711 - val_accuracy: 0.8841 - lr: 4.0000e-04\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9079\n",
      "Epoch 17: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 125s 398ms/step - loss: 0.2684 - accuracy: 0.9079 - val_loss: 0.3697 - val_accuracy: 0.8854 - lr: 4.0000e-04\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.9089\n",
      "Epoch 18: val_accuracy did not improve from 0.88670\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00019999999494757503.\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 126s 402ms/step - loss: 0.2678 - accuracy: 0.9089 - val_loss: 0.3725 - val_accuracy: 0.8846 - lr: 4.0000e-04\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9073\n",
      "Epoch 19: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.2676 - accuracy: 0.9073 - val_loss: 0.3708 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9089\n",
      "Epoch 20: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 130s 416ms/step - loss: 0.2682 - accuracy: 0.9089 - val_loss: 0.3719 - val_accuracy: 0.8849 - lr: 2.0000e-04\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.9087\n",
      "Epoch 21: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 129s 412ms/step - loss: 0.2693 - accuracy: 0.9087 - val_loss: 0.3725 - val_accuracy: 0.8858 - lr: 2.0000e-04\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9095\n",
      "Epoch 22: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 125s 400ms/step - loss: 0.2662 - accuracy: 0.9095 - val_loss: 0.3708 - val_accuracy: 0.8853 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.9099\n",
      "Epoch 23: val_accuracy did not improve from 0.88670\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 126s 403ms/step - loss: 0.2670 - accuracy: 0.9099 - val_loss: 0.3691 - val_accuracy: 0.8855 - lr: 2.0000e-04\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.9083\n",
      "Epoch 24: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 123s 393ms/step - loss: 0.2660 - accuracy: 0.9083 - val_loss: 0.3696 - val_accuracy: 0.8846 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9090\n",
      "Epoch 25: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.2696 - accuracy: 0.9090 - val_loss: 0.3699 - val_accuracy: 0.8860 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.9071\n",
      "Epoch 26: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 386ms/step - loss: 0.2727 - accuracy: 0.9071 - val_loss: 0.3693 - val_accuracy: 0.8863 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9058\n",
      "Epoch 27: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 121s 388ms/step - loss: 0.2712 - accuracy: 0.9058 - val_loss: 0.3693 - val_accuracy: 0.8860 - lr: 1.0000e-04\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.9055\n",
      "Epoch 28: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 128s 410ms/step - loss: 0.2738 - accuracy: 0.9055 - val_loss: 0.3711 - val_accuracy: 0.8861 - lr: 1.0000e-04\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2752 - accuracy: 0.9061\n",
      "Epoch 29: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 126s 402ms/step - loss: 0.2752 - accuracy: 0.9061 - val_loss: 0.3706 - val_accuracy: 0.8860 - lr: 1.0000e-04\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9080\n",
      "Epoch 30: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 129s 411ms/step - loss: 0.2671 - accuracy: 0.9080 - val_loss: 0.3713 - val_accuracy: 0.8848 - lr: 1.0000e-04\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9076\n",
      "Epoch 31: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 128s 408ms/step - loss: 0.2677 - accuracy: 0.9076 - val_loss: 0.3691 - val_accuracy: 0.8861 - lr: 1.0000e-04\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9073\n",
      "Epoch 32: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 120s 381ms/step - loss: 0.2664 - accuracy: 0.9073 - val_loss: 0.3714 - val_accuracy: 0.8846 - lr: 1.0000e-04\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9083\n",
      "Epoch 33: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 121s 387ms/step - loss: 0.2651 - accuracy: 0.9083 - val_loss: 0.3700 - val_accuracy: 0.8860 - lr: 1.0000e-04\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9071\n",
      "Epoch 34: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 122s 391ms/step - loss: 0.2662 - accuracy: 0.9071 - val_loss: 0.3691 - val_accuracy: 0.8857 - lr: 1.0000e-04\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9075\n",
      "Epoch 35: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 395ms/step - loss: 0.2682 - accuracy: 0.9075 - val_loss: 0.3694 - val_accuracy: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.9082\n",
      "Epoch 36: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 123s 394ms/step - loss: 0.2688 - accuracy: 0.9082 - val_loss: 0.3701 - val_accuracy: 0.8856 - lr: 1.0000e-04\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9079\n",
      "Epoch 37: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 124s 397ms/step - loss: 0.2675 - accuracy: 0.9079 - val_loss: 0.3685 - val_accuracy: 0.8859 - lr: 1.0000e-04\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.9069\n",
      "Epoch 38: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 125s 400ms/step - loss: 0.2670 - accuracy: 0.9069 - val_loss: 0.3690 - val_accuracy: 0.8859 - lr: 1.0000e-04\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9058\n",
      "Epoch 39: val_accuracy did not improve from 0.88670\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 127s 406ms/step - loss: 0.2721 - accuracy: 0.9058 - val_loss: 0.3690 - val_accuracy: 0.8853 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#Using pickle files to generate x_train & y_train, so that there is no data leakage\n",
    "nadam = tf.keras.optimizers.legacy.Nadam(learning_rate=8.0000e-04, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=1e-4)\n",
    "\n",
    "model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "logs='logs/model_1'\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir=logs)\n",
    "\n",
    "model.fit(train_datagen.flow(x_train, y_train,batch_size=128),epochs=50, validation_data=(x_cv, y_cv), callbacks=[tensorboard,checkpoint_save,learning_rate_reduction,CustomCallback(x_cv, y_cv)], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('saved_models/final_models/f_model_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 23s 39ms/step - loss: 0.3799 - accuracy: 0.8822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3799421787261963, 0.8822000026702881]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-396dd11e6b7edda6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-396dd11e6b7edda6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/model_1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
