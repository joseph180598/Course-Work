{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDMgSstPYv0P"
   },
   "source": [
    "# Text Classification:\n",
    "\n",
    "## Data\n",
    "<pre>\n",
    "1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n",
    "2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \n",
    "so from document name, you can extract the label for that document.\n",
    "4. Now our problem is to classify all the documents into any one of the class.\n",
    "5. Below we provided count plot of all the labels in our data. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "64U9NzWFYv0V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "### count plot of all the class labels. \n",
    "import os\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.chunk import ne_chunk\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxwha_caXp2u",
    "outputId": "4851c296-940a-4bf0-c31c-eccf3552e89c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I4l1s98XeBX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckB6VzXDXexL",
    "outputId": "2939a1ca-f652-4e60-ca32-7e9c3b07a910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mK4TJOFYv0h"
   },
   "source": [
    "## Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlqYFVI3Yv0k"
   },
   "source": [
    "#### sample document\n",
    "<pre>\n",
    "<font color='blue'>\n",
    "Subject: A word of advice\n",
    "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
    "\n",
    "In article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n",
    ">\n",
    ">I've said 100 times that there is no \"alternative\" that should think you\n",
    ">might have caught on by now.  And there is no \"alternative\", but the point\n",
    ">is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n",
    ">and religious knowledge are unsolvable-- or I should say, humans cannot\n",
    ">solve them.\n",
    "\n",
    "How does that saying go: Those who say it can't be done shouldn't interrupt\n",
    "those who are doing it.\n",
    "\n",
    "Jim\n",
    "--\n",
    "Have you washed your brain today?\n",
    "</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAR5HoR1Yv0m"
   },
   "source": [
    "### Preprocessing:\n",
    "<pre>\n",
    "useful links: <a href='http://www.pyregex.com/'>http://www.pyregex.com/</a>\n",
    "\n",
    "<font color='blue'><b>1.</b></font> Find all emails in the document and then get the text after the \"@\". and then split those texts by '.' \n",
    "after that remove the words whose length is less than or equal to 2 and also remove'com' word and then combine those words by space. \n",
    "In one doc, if we have 2 or more mails, get all.\n",
    "<b>Eg:[test@dm1.d.com, test2@dm2.dm3.com]-->[dm1.d.com, dm3.dm4.com]-->[dm1,d,com,dm2,dm3,com]-->[dm1,dm2,dm3]-->\"dm1 dm2 dm3\" </b> \n",
    "append all those into one list/array. ( This will give length of 18828 sentences i.e one list for each of the document). \n",
    "Some sample output was shown below. \n",
    "\n",
    "> In the above sample document there are emails [jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu]\n",
    "\n",
    "preprocessing:\n",
    "[jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu] ==> [nyx cs du edu mimsy umd edu cs umd edu] ==> \n",
    "[nyx edu mimsy umd edu umd edu]\n",
    "\n",
    "<font color='blue'><b>2.</b></font> Replace all the emails by space in the original text. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KavKDD9FYv0p",
    "outputId": "0b87ab7b-46df-4995-eaca-4f5831ad223e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['juliet caltech edu',\n",
       "       'coding bchs edu newsgate sps mot austlcm sps mot austlcm sps mot com  dna bchs edu',\n",
       "       'batman bmd trw', ..., 'rbdc wsnc org dscomsa desy zeus  desy',\n",
       "       'rbdc wsnc org morrow stanford edu pangea Stanford EDU',\n",
       "       'rbdc wsnc org apollo apollo'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have collected all emails and preprocessed them, this is sample output\n",
    "preprocessed_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obReqs55Yv0v",
    "outputId": "10770414-9be0-4d63-9587-5363a8c10c4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18828"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIovFDQzYv03"
   },
   "source": [
    "<pre>\n",
    "<font color='blue'><b>3.</b></font> Get subject of the text i.e. get the total lines where \"Subject:\" occur and remove \n",
    "the word which are before the \":\" remove the newlines, tabs, punctuations, any special chars.\n",
    "<b>Eg: if we have sentance like \"Subject: Re: Gospel Dating @ \\r\\r\\n\" --> You have to get \"Gospel Dating\"</b> \n",
    "Save all this data into another list/array. \n",
    "\n",
    "<font color='blue'><b>4.</b></font> After you store it in the list, Replace those sentances in original text by space.\n",
    "\n",
    "<font color='blue'><b>5.</b></font> Delete all the sentances where sentence starts with <b>\"Write to:\"</b> or <b>\"From:\"</b>.\n",
    "> In the above sample document check the 2nd line, we should remove that\n",
    "\n",
    "<font color='blue'><b>6.</b></font> Delete all the tags like \"< anyword >\"\n",
    "> In the above sample document check the 4nd line, we should remove that \"< 65882@mimsy.umd.edu >\"\n",
    "\n",
    "\n",
    "<font color='blue'><b>7.</b></font> Delete all the data which are present in the brackets. \n",
    "In many text data, we observed that, they maintained the explanation of sentence \n",
    "or translation of sentence to another language in brackets so remove all those.\n",
    "<b>Eg: \"AAIC-The course that gets you HIRED(AAIC - Der Kurs, der Sie anstellt)\" --> \"AAIC-The course that gets you HIRED\"</b>\n",
    "\n",
    "> In the above sample document check the 4nd line, we should remove that \"(Charley Wingate)\"\n",
    "\n",
    "\n",
    "<font color='blue'><b>8.</b></font> Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".\n",
    "\n",
    "<font color='blue'><b>9.</b></font> Remove all the words which ends with <b>\":\"</b>.\n",
    "<b>Eg: \"Anyword:\"</b>\n",
    "> In the above sample document check the 4nd line, we should remove that \"writes:\"\n",
    "\n",
    "\n",
    "<font color='blue'><b>10.</b></font> Decontractions, replace words like below to full words. \n",
    "please check the donors choose preprocessing for this \n",
    "<b>Eg: can't -> can not, 's -> is, i've -> i have, i'm -> i am, you're -> you are, i'll --> i will </b>\n",
    "\n",
    "<b> There is no order to do point 6 to 10. but you have to get final output correctly</b>\n",
    "\n",
    "<font color='blue'><b>11.</b></font> Do chunking on the text you have after above preprocessing. \n",
    "Text chunking, also referred to as shallow parsing, is a task that \n",
    "follows Part-Of-Speech Tagging and that adds more structure to the sentence.\n",
    "So it combines the some phrases, named entities into single word.\n",
    "So after that combine all those phrases/named entities by separating <b>\"_\"</b>. \n",
    "And remove the phrases/named entities if that is a \"Person\". \n",
    "You can use <b>nltk.ne_chunk</b> to get these. \n",
    "Below we have given one example. please go through it. \n",
    "\n",
    "useful links: \n",
    "<a href='https://www.nltk.org/book/ch07.html'>https://www.nltk.org/book/ch07.html</a>\n",
    "<a href='https://stackoverflow.com/a/31837224/4084039'>https://stackoverflow.com/a/31837224/4084039</a>\n",
    "<a href='http://www.nltk.org/howto/tree.html'>http://www.nltk.org/howto/tree.html</a>\n",
    "<a href='https://stackoverflow.com/a/44294377/4084039'>https://stackoverflow.com/a/44294377/4084039</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lAaKQ6EYv04",
    "outputId": "53b66a94-acef-4002-e51c-002bde4178b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am living in the New York --> [('i', 'NN'), ('am', 'VBP'), ('living', 'VBG'), ('in', 'IN'), ('the', 'DT'), Tree('GPE', [('New', 'NNP'), ('York', 'NNP')])]\n",
      " \n",
      "--------------------------------------------------\n",
      " \n",
      "My name is Srikanth Varma --> [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), Tree('PERSON', [('Srikanth', 'NNP'), ('Varma', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "#i am living in the New York\n",
    "print(\"i am living in the New York -->\", list(chunks))\n",
    "print(\" \")\n",
    "print(\"-\"*50)\n",
    "print(\" \")\n",
    "#My name is Srikanth Varma\n",
    "print(\"My name is Srikanth Varma -->\", list(chunks1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV8gzLUjYv0-"
   },
   "source": [
    "<pre>We did chunking for above two lines and then We got one list where each word is mapped to a \n",
    "POS(parts of speech) and also if you see \"New York\" and \"Srikanth Varma\", \n",
    "they got combined and represented as a tree and \"New York\" was referred as \"GPE\" and \"Srikanth Varma\" was referred as \"PERSON\". \n",
    "so now you have to Combine the \"New York\" with <b>\"_\"</b> i.e \"New_York\"\n",
    "and remove the \"Srikanth Varma\" from the above sentence because it is a person.</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpaC-KF3Yv1A"
   },
   "source": [
    "<pre>\n",
    "<font color='blue'><b>13.</b></font> Replace all the digits with space i.e delete all the digits. \n",
    "> In the above sample document, the 6th line have digit 100, so we have to remove that.\n",
    "\n",
    "<font color='blue'><b>14.</b></font> After doing above points, we observed there might be few word's like\n",
    " <b> \"_word_\" (i.e starting and ending with the _), \"_word\" (i.e starting with the _),\n",
    "  \"word_\" (i.e ending with the _)</b> remove the <b>_</b> from these type of words. \n",
    "\n",
    "<font color='blue'><b>15.</b></font>  We also observed some words like <b> \"OneLetter_word\"- eg: d_berlin, \n",
    "\"TwoLetters_word\" - eg: dr_berlin </b>, in these words we remove the \"OneLetter_\" (d_berlin ==> berlin) and \n",
    "\"TwoLetters_\" (de_berlin ==> berlin). i.e remove the words \n",
    "which are length less than or equal to 2 after spliiting those words by \"_\". \n",
    "\n",
    "<font color='blue'><b>16.</b></font> Convert all the words into lower case and lowe case \n",
    "and remove the words which are greater than or equal to 15 or less than or equal to 2.\n",
    "\n",
    "<font color='blue'><b>17.</b></font> replace all the words except \"A-Za-z_\" with space. \n",
    "\n",
    "<font color='blue'><b>18.</b></font> Now You got Preprocessed Text, email, subject. create a dataframe with those. \n",
    "Below are the columns of the df. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB43OGEfYv1C",
    "outputId": "945bc8a4-1f99-4410-94c8-c776a405b5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'class', 'preprocessed_text', 'preprocessed_subject',\n",
      "       'preprocessed_emails'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM6A19xFYv1I",
    "outputId": "9de13fa8-6604-49a2-8013-6b22f0a256a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                    From: arc1@ukc.ac.uk (Tony Curtis)\\r\\r\\r\\nSubj...\n",
      "class                                                         alt.atheism\n",
      "preprocessed_text       said re is article if followed the quoting rig...\n",
      "preprocessed_subject                                christian morality is\n",
      "preprocessed_emails                                   ukc mac macalstr edu\n",
      "Name: 567, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data.iloc[400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfWUeIN1Yv1N"
   },
   "source": [
    "### To get above mentioned data frame --> Try to Write Total Preprocessing steps in One Function Named Preprocess as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "uEGEHTNQYv1N"
   },
   "outputs": [],
   "source": [
    "def preprocess(Input_Text):\n",
    "    \"\"\"Do all the Preprocessing as shown above and\n",
    "    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data\"\"\"\n",
    "    \n",
    "    pattern=r'([a-zA-Z0-9\\-\\.]+@[a-zA-z\\.]{2,})'\n",
    "    email_id=re.findall(pattern, Input_Text, flags=re.M|re.I)\n",
    "    \n",
    "    email_file=[]\n",
    "    for ids in email_id:\n",
    "        e_ids=ids.split('@')[1]\n",
    "        #print(e_ids)\n",
    "        domain=e_ids.split('.')\n",
    "        domain=' '.join(list(d for d in domain if (len(d)>2 and d!='com')))\n",
    "        email_file.append(domain)\n",
    "     \n",
    "    email_file=' '.join(email_file)\n",
    "    #print(email_file)\n",
    "    \n",
    "    #Replacing email with space\n",
    "    pattern=r'([a-zA-Z0-9\\-\\.]+@[a-zA-z\\.]{2,})'\n",
    "    Input_Text=re.sub(pattern,' ',Input_Text)\n",
    "    \n",
    "    #Input_Text\n",
    "    \n",
    "    #Extracting the subject>Removing any word before :>Then remove any character other than a-zA-Z\n",
    "    pattern_1=r'subject:.+'\n",
    "    data=re.findall(pattern_1, Input_Text, flags=re.M|re.I)\n",
    "    pattern_2=r'[a-zA-Z]+:'\n",
    "    text=re.sub(pattern_2,'',data[0])\n",
    "    pattern_3=r'[^a-zA-z]'\n",
    "    text=re.sub(pattern_3,' ',text,flags=re.M|re.I)\n",
    "    \n",
    "    subject=text.lower()\n",
    "    \n",
    "    #Removing the whole content from the text\n",
    "    pattern_1=r'(subject:.+)|(From:.+)|(Write to:.+)'\n",
    "    Input_Text=re.sub(pattern_1,' ',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_3=r'(<.+?>)|(\\(.+?\\))|(\\(.+?\\))'\n",
    "    Input_Text=re.sub(pattern_3,'',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_4=r'\\b[a-z]+:'\n",
    "    Input_Text=re.sub(pattern_4,'',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_5=r'[\\r\\n]|[\\r\\t]'\n",
    "    Input_Text=re.sub(pattern_5,' ',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_6=r'>+|\\\\+|-+|\\\\|\\\"|,|\\?'\n",
    "    Input_Text=re.sub(pattern_6,'',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_8=r'\\'ve'\n",
    "    Input_Text=re.sub(pattern_8,' have',Input_Text,flags=re.M|re.I)\n",
    "\n",
    "    pattern_9=r'can\\'t'\n",
    "    Input_Text=re.sub(pattern_9,'can not',Input_Text,flags=re.M|re.I)\n",
    "\n",
    "    pattern_10=r'\\'s'\n",
    "    Input_Text=re.sub(pattern_10,'is',Input_Text,flags=re.M|re.I)\n",
    "\n",
    "    pattern_11=r'i\\'ve'\n",
    "    Input_Text=re.sub(pattern_11,'have',Input_Text,flags=re.M|re.I)\n",
    "\n",
    "    pattern_11=r'i\\'m'\n",
    "    Input_Text=re.sub(pattern_11,'i am',Input_Text,flags=re.M|re.I)\n",
    "\n",
    "    pattern_11=r'you\\'re'\n",
    "    Input_Text=re.sub(pattern_11,'you are',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_11=r'i\\'ll'\n",
    "    Input_Text=re.sub(pattern_11,'i will',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_11=r'\\'ll'\n",
    "    Input_Text=re.sub(pattern_11,' will',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    tokens=nltk.word_tokenize(Input_Text)\n",
    "    pos=nltk.pos_tag(tokens)\n",
    "    chunks=ne_chunk(pos, binary=False)\n",
    "\n",
    "    named_entities=[]\n",
    "\n",
    "    for t in chunks.subtrees():\n",
    "        if t.label() == 'PERSON':\n",
    "            named_entities.append(list(t))\n",
    "        \n",
    "    for entity in named_entities:\n",
    "        word=[entity[x][0] for x in range(len(entity))]\n",
    "        pattern=re.escape(str(' '.join(word)))\n",
    "        #print(pattern)\n",
    "        Input_Text=re.sub(pattern,'',Input_Text)\n",
    "    \n",
    "\n",
    "    for t in chunks.subtrees():\n",
    "        if t.label() == 'GPE':\n",
    "            named_entities.append(list(t))\n",
    "        \n",
    "    for entity in named_entities:\n",
    "        word=[entity[x][0] for x in range(len(entity))]\n",
    "        pattern=re.escape(str(' '.join(word)))\n",
    "        Input_Text=re.sub(pattern,'_'.join(word),Input_Text)\n",
    "    \n",
    "    \n",
    "    pattern_12=r'_[a-zA-Z]+_'\n",
    "    values=re.findall(pattern_12,Input_Text,flags=re.M|re.I)\n",
    "    for x in values:\n",
    "        i=x.split('_')\n",
    "        i=[x for x in i if len(x)>=2]\n",
    "        for word in i:\n",
    "            Input_Text=re.sub(x,word,Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_12=r'[a-zA-Z]+_ '\n",
    "    values=re.findall(pattern_12,Input_Text,flags=re.M|re.I)\n",
    "    for x in values:\n",
    "        i=x.split('_')\n",
    "        i=[x for x in i if len(x)>=2]\n",
    "        for word in i:\n",
    "            Input_Text=re.sub(x,word+' ',Input_Text,flags=re.M|re.I)\n",
    "         \n",
    "    pattern_12=r'[a-zA-Z]{0,2}_[a-zA-Z]+'\n",
    "    values=re.findall(pattern_12,Input_Text,flags=re.M|re.I)\n",
    "    for x in values:\n",
    "        i=x.split('_')\n",
    "        i=[x for x in i if len(x)>2]\n",
    "        for word in i:\n",
    "            if len(i)==0:\n",
    "                Input_Text=re.sub(x,' ',Input_Text,flags=re.M|re.I)\n",
    "            else:\n",
    "                Input_Text=re.sub(x,str(word+' '),Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_12=r'(\\b\\w{1}\\b)|(\\w{15,})'\n",
    "    Input_Text=re.sub(pattern_12,'',Input_Text,flags=re.M|re.I)\n",
    "\n",
    "    pattern_12=r'[^a-zA-Z_]'\n",
    "    Input_Text=re.sub(pattern_12,' ',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_7=r' +'\n",
    "    Input_Text=re.sub(pattern_7,' ',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    pattern_11=r' \\w{1} '\n",
    "    Input_Text=re.sub(pattern_11,'',Input_Text,flags=re.M|re.I)\n",
    "    \n",
    "    processed_text=Input_Text.lower()\n",
    "    return (email_file,subject,processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "tlriZdhtXL2U",
    "outputId": "4058c44e-56b3-42f1-962d-fd745883e4df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mantis netcom mantis',\n",
       " ' alt atheism  atheist resources',\n",
       " ' archive atheism resources resources last december atheist resources addresses of atheist organizations usa freedom from religion foundation fish bumper stickers and assorted other atheist paraphernalia are available from the freedom from religion foundation in the us evolution designs evolution designs sell the fish itis fish symbol like the ones stick on their cars but with feet and the word written inside the deluxe mouldedplastic fish is postpaid in the us ca people in the san francisco bay area can get fish from try mailing for net people who go to directly the price is per fish american atheist press aap publish various atheist books critiques of the bible lists of biblical contradictions and so on one such book the bible handbook by and american atheist press pp isbn nd edition bible contradictions absurdities atrocities immoralities contains the bible contradicts itself aap based on the king version of the bible tx prometheus books sell books including an alternate address prometheus books ny for humanism an organization promoting black secular humanism and uncovering the history of black freethought they publish quarterly newsletter aah examiner ny united kingdom rationalist press association national secular society street holloway roadewnl british humanist association south place ethical society lambis conduit passage conway hall wcrh red lion square wcrl fax the national secular society publish the freethinker monthly magazine founded in germany ibka bund der undberlin germany ibka publish miz materialien und zur zeit politisches journal der und ibka mizvertrieb postfachberlin germany for atheist books write ibdk bucherdienst derhannover germany books fiction thomas disch the claus compromise short story the ultimate proof that exists all characters and events are fictitious any similarity to living or dead gods uh well walter canticle for leibowitz one gem in this post atomic doomsday novel is the monks who spent their lives copying blueprints from filling the sheets of paper with ink and leaving white lines and letters edgar pangborn atomic doomsday novel set in clerical states the church for example forbids that anyone produce describe or use any substance containing atoms philip dick wrote many philosophical and short stories and novels his stories are bizarre at times but very approachable he wrote mainly sf but he wrote about people truth and religion rather than technology although he often believed that he had met some sort of he remained sceptical amongst his novels the following are of some galactic pothealer fallible alien deity summons group of craftsmen and women to remote planet to raise giant cathedral from beneath the oceans when the deity begins to demand faith from the earthers pothealer is unable to comply polished ironic and amusing novel maze of for its description of religion valis the schizophrenic hero searches for the hidden mysteries of gnostic ity after reality is fired into his brain by pink laser beam of unknown but possibly divine origin he is accompanied by his dogmatic and dismissively atheist friend and assorted other odd characters the divine invasion invades by making young woman pregnant as she returns from another star system unfortunately she is terminally ill and must be assisted by dead man whose brain is wired to hour easy listening music margaret atwood the handmaidis tale story based on the premise that the us congress is mysteriously assassinated and quickly take charge of the nation to set it right again the book is the diary of womanis life as she tries to live under the new theocracy right to own property is revoked and their bank accounts are closed sinful luxuries are outlawed and the radio is only used for readings from the bible crimes are punished doctors who performed legal abortions in the old world are hunted down and hanged atwoodis writing style is difficult to get used to at first but the tale grows more and more chilling as it goes on various authors the bible this somewhat dull and rambling work has often been criticized however it is probably worth reading if only so that you will know what all the fuss is about it exists in many different versions so make sure you get the one true version books nonfiction peter de rosa vicars of christ bantam press although de seems to be or even catholic this is very enlighting history of papal immoralities adulteries fallacies etc german gottes erste dunkle seite des droemerknaur michael martin philosophical justification temple university press philadelphia usa detailed and scholarly justification of atheism contains an outstanding appendix defining terminology and usage in this tendentious area argues both for negative atheism the nonbelief in the existence of god and also for positive atheism the belief in the nonexistence of god includes great refutations of the most challenging arguments for god particular attention is paid to refuting contempory theists such as and swinburne pages isbn the case against ity temple university press comprehensive critique of ity in which he considers the best contemporary defences of ity and demonstrates that they are unsupportable and or incoherent pages isbn james turner without the johns hopkins university press baltimore md usa subtitled the origins of unbelief in america examines the way in which unbelief became mainstream alternative worldview focusses on the period and while considering france and britain the emphasis is on american and particularly nengland developments neither religious history of secularization or atheism without is rather the intellectual history of the fate of single idea the belief that exists pages isbngeorge seldes the great thoughts antine books new york usa dictionary of quotations of different kind concentrating on statements and writings which explicitly or implicitly present the personis philosophy and worldview includes obscure opinions from many people for some popular observations traces the way in which various people expressed and twisted the idea over the centuries quite number of the quotations are derived from what of religion and views of religion pages isbnswinburne the existence of clarendon paperbacks oxford this book is the second volume in trilogy that began with the coherence of theism and was concluded with and in this work swinburne attempts to construct series of inductive arguments for the existence of his arguments which are somewhat tendentious and rely upon the imputation of late th century western values and aesthetics to which is supposedly as simple as can be conceived were decisively rejected in is the miracle of theism in the revised edition of the existence of swinburne includes an appendix in which he makes somewhat incoherent attempt to rebut mackie the miracle of theism oxford this volume contains comprehensive review of the principal arguments for and against the existence of it ranges from the classical philosophical positions of et al through the moral arguments of newman kant and to the recent restatements of the classical theses by and swinburne it also addresses those positions which push the concept of beyond the realm of the rational such as those of kierkegaard kung and as well as replacements for such as lelieis axiarchism the book is delight to read less formalistic and better written than works and refreshingly direct when compared with the handwaving of swinburne james haught holy an illustrated history of religious murder and madness prometheus books looks at religious persecution from ancient times to the present day and not only by library of congress catalog card number norm allen jr african american an the listing for african americans for humanism above gordon stein an anthology of atheism and anthology covering wide range of subjects including the devil evil and morality and the history of freethought comprehensive bibliography edmund cohen the mind of the biblebeliever prometheus books study of why people become and what effect it has on them net resources thereis small mailbased archive server at mantis co uk which carries archives of old alt atheism moderated articles and assorted other files for more information send mail to saying help send atheism index and it will mail back reply mathew ')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=open('documents/alt.atheism_49960.txt')\n",
    "text=data.read()\n",
    "preprocess(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: mathew <mathew@mantis.co.uk>\\nSubject: Alt.Atheism FAQ: Atheist Resources\\n\\nArchive-name: atheism/resources\\nAlt-atheism-archive-name: resources\\nLast-modified: 11 December 1992\\nVersion: 1.0\\n\\n                              Atheist Resources\\n\\n                      Addresses of Atheist Organizations\\n\\n                                     USA\\n\\nFREEDOM FROM RELIGION FOUNDATION\\n\\nDarwin fish bumper stickers and assorted other atheist paraphernalia are\\navailable from the Freedom From Religion Foundation in the US.\\n\\nWrite to:  FFRF, P.O. Box 750, Madison, WI 53701.\\nTelephone: (608) 256-8900\\n\\nEVOLUTION DESIGNS\\n\\nEvolution Designs sell the \"Darwin fish\".  It\\'s a fish symbol, like the ones\\nChristians stick on their cars, but with feet and the word \"Darwin\" written\\ninside.  The deluxe moulded 3D plastic fish is $4.95 postpaid in the US.\\n\\nWrite to:  Evolution Designs, 7119 Laurel Canyon #4, North Hollywood,\\n           CA 91605.\\n\\nPeople in the San Francisco Bay area can get Darwin Fish from Lynn Gold --\\ntry mailing <figmo@netcom.com>.  For net people who go to Lynn directly, the\\nprice is $4.95 per fish.\\n\\nAMERICAN ATHEIST PRESS\\n\\nAAP publish various atheist books -- critiques of the Bible, lists of\\nBiblical contradictions, and so on.  One such book is:\\n\\n\"The Bible Handbook\" by W.P. Ball and G.W. Foote.  American Atheist Press.\\n372 pp.  ISBN 0-910309-26-4, 2nd edition, 1986.  Bible contradictions,\\nabsurdities, atrocities, immoralities... contains Ball, Foote: \"The Bible\\nContradicts Itself\", AAP.  Based on the King James version of the Bible.\\n\\nWrite to:  American Atheist Press, P.O. Box 140195, Austin, TX 78714-0195.\\n      or:  7215 Cameron Road, Austin, TX 78752-2973.\\nTelephone: (512) 458-1244\\nFax:       (512) 467-9525\\n\\nPROMETHEUS BOOKS\\n\\nSell books including Haught\\'s \"Holy Horrors\" (see below).\\n\\nWrite to:  700 East Amherst Street, Buffalo, New York 14215.\\nTelephone: (716) 837-2475.\\n\\nAn alternate address (which may be newer or older) is:\\nPrometheus Books, 59 Glenn Drive, Buffalo, NY 14228-2197.\\n\\nAFRICAN-AMERICANS FOR HUMANISM\\n\\nAn organization promoting black secular humanism and uncovering the history of\\nblack freethought.  They publish a quarterly newsletter, AAH EXAMINER.\\n\\nWrite to:  Norm R. Allen, Jr., African Americans for Humanism, P.O. Box 664,\\n           Buffalo, NY 14226.\\n\\n                                United Kingdom\\n\\nRationalist Press Association          National Secular Society\\n88 Islington High Street               702 Holloway Road\\nLondon N1 8EW                          London N19 3NL\\n071 226 7251                           071 272 1266\\n\\nBritish Humanist Association           South Place Ethical Society\\n14 Lamb\\'s Conduit Passage              Conway Hall\\nLondon WC1R 4RH                        Red Lion Square\\n071 430 0908                           London WC1R 4RL\\nfax 071 430 1271                       071 831 7723\\n\\nThe National Secular Society publish \"The Freethinker\", a monthly magazine\\nfounded in 1881.\\n\\n                                   Germany\\n\\nIBKA e.V.\\nInternationaler Bund der Konfessionslosen und Atheisten\\nPostfach 880, D-1000 Berlin 41. Germany.\\n\\nIBKA publish a journal:\\nMIZ. (Materialien und Informationen zur Zeit. Politisches\\nJournal der Konfessionslosesn und Atheisten. Hrsg. IBKA e.V.)\\nMIZ-Vertrieb, Postfach 880, D-1000 Berlin 41. Germany.\\n\\nFor atheist books, write to:\\n\\nIBDK, Internationaler B\"ucherdienst der Konfessionslosen\\nPostfach 3005, D-3000 Hannover 1. Germany.\\nTelephone: 0511/211216\\n\\n\\n                               Books -- Fiction\\n\\nTHOMAS M. DISCH\\n\\n\"The Santa Claus Compromise\"\\nShort story.  The ultimate proof that Santa exists.  All characters and \\nevents are fictitious.  Any similarity to living or dead gods -- uh, well...\\n\\nWALTER M. MILLER, JR\\n\\n\"A Canticle for Leibowitz\"\\nOne gem in this post atomic doomsday novel is the monks who spent their lives\\ncopying blueprints from \"Saint Leibowitz\", filling the sheets of paper with\\nink and leaving white lines and letters.\\n\\nEDGAR PANGBORN\\n\\n\"Davy\"\\nPost atomic doomsday novel set in clerical states.  The church, for example,\\nforbids that anyone \"produce, describe or use any substance containing...\\natoms\". \\n\\nPHILIP K. DICK\\n\\nPhilip K. Dick Dick wrote many philosophical and thought-provoking short \\nstories and novels.  His stories are bizarre at times, but very approachable.\\nHe wrote mainly SF, but he wrote about people, truth and religion rather than\\ntechnology.  Although he often believed that he had met some sort of God, he\\nremained sceptical.  Amongst his novels, the following are of some relevance:\\n\\n\"Galactic Pot-Healer\"\\nA fallible alien deity summons a group of Earth craftsmen and women to a\\nremote planet to raise a giant cathedral from beneath the oceans.  When the\\ndeity begins to demand faith from the earthers, pot-healer Joe Fernwright is\\nunable to comply.  A polished, ironic and amusing novel.\\n\\n\"A Maze of Death\"\\nNoteworthy for its description of a technology-based religion.\\n\\n\"VALIS\"\\nThe schizophrenic hero searches for the hidden mysteries of Gnostic\\nChristianity after reality is fired into his brain by a pink laser beam of\\nunknown but possibly divine origin.  He is accompanied by his dogmatic and\\ndismissively atheist friend and assorted other odd characters.\\n\\n\"The Divine Invasion\"\\nGod invades Earth by making a young woman pregnant as she returns from\\nanother star system.  Unfortunately she is terminally ill, and must be\\nassisted by a dead man whose brain is wired to 24-hour easy listening music.\\n\\nMARGARET ATWOOD\\n\\n\"The Handmaid\\'s Tale\"\\nA story based on the premise that the US Congress is mysteriously\\nassassinated, and fundamentalists quickly take charge of the nation to set it\\n\"right\" again.  The book is the diary of a woman\\'s life as she tries to live\\nunder the new Christian theocracy.  Women\\'s right to own property is revoked,\\nand their bank accounts are closed; sinful luxuries are outlawed, and the\\nradio is only used for readings from the Bible.  Crimes are punished\\nretroactively: doctors who performed legal abortions in the \"old world\" are\\nhunted down and hanged.  Atwood\\'s writing style is difficult to get used to\\nat first, but the tale grows more and more chilling as it goes on.\\n\\nVARIOUS AUTHORS\\n\\n\"The Bible\"\\nThis somewhat dull and rambling work has often been criticized.  However, it\\nis probably worth reading, if only so that you\\'ll know what all the fuss is\\nabout.  It exists in many different versions, so make sure you get the one\\ntrue version.\\n\\n                             Books -- Non-fiction\\n\\nPETER DE ROSA\\n\\n\"Vicars of Christ\", Bantam Press, 1988\\nAlthough de Rosa seems to be Christian or even Catholic this is a very\\nenlighting history of papal immoralities, adulteries, fallacies etc.\\n(German translation: \"Gottes erste Diener. Die dunkle Seite des Papsttums\",\\nDroemer-Knaur, 1989)\\n\\nMICHAEL MARTIN\\n\\n\"Atheism: A Philosophical Justification\", Temple University Press,\\n Philadelphia, USA.\\nA detailed and scholarly justification of atheism.  Contains an outstanding\\nappendix defining terminology and usage in this (necessarily) tendentious\\narea.  Argues both for \"negative atheism\" (i.e. the \"non-belief in the\\nexistence of god(s)\") and also for \"positive atheism\" (\"the belief in the\\nnon-existence of god(s)\").  Includes great refutations of the most\\nchallenging arguments for god; particular attention is paid to refuting\\ncontempory theists such as Platinga and Swinburne.\\n541 pages. ISBN 0-87722-642-3 (hardcover; paperback also available)\\n\\n\"The Case Against Christianity\", Temple University Press\\nA comprehensive critique of Christianity, in which he considers\\nthe best contemporary defences of Christianity and (ultimately)\\ndemonstrates that they are unsupportable and/or incoherent.\\n273 pages. ISBN 0-87722-767-5\\n\\nJAMES TURNER\\n\\n\"Without God, Without Creed\", The Johns Hopkins University Press, Baltimore,\\n MD, USA\\nSubtitled \"The Origins of Unbelief in America\".  Examines the way in which\\nunbelief (whether agnostic or atheistic)  became a mainstream alternative\\nworld-view.  Focusses on the period 1770-1900, and while considering France\\nand Britain the emphasis is on American, and particularly New England\\ndevelopments.  \"Neither a religious history of secularization or atheism,\\nWithout God, Without Creed is, rather, the intellectual history of the fate\\nof a single idea, the belief that God exists.\" \\n316 pages. ISBN (hardcover) 0-8018-2494-X (paper) 0-8018-3407-4\\n\\nGEORGE SELDES (Editor)\\n\\n\"The great thoughts\", Ballantine Books, New York, USA\\nA \"dictionary of quotations\" of a different kind, concentrating on statements\\nand writings which, explicitly or implicitly, present the person\\'s philosophy\\nand world-view.  Includes obscure (and often suppressed) opinions from many\\npeople.  For some popular observations, traces the way in which various\\npeople expressed and twisted the idea over the centuries.  Quite a number of\\nthe quotations are derived from Cardiff\\'s \"What Great Men Think of Religion\"\\nand Noyes\\' \"Views of Religion\".\\n490 pages. ISBN (paper) 0-345-29887-X.\\n\\nRICHARD SWINBURNE\\n\\n\"The Existence of God (Revised Edition)\", Clarendon Paperbacks, Oxford\\nThis book is the second volume in a trilogy that began with \"The Coherence of\\nTheism\" (1977) and was concluded with \"Faith and Reason\" (1981).  In this\\nwork, Swinburne attempts to construct a series of inductive arguments for the\\nexistence of God.  His arguments, which are somewhat tendentious and rely\\nupon the imputation of late 20th century western Christian values and\\naesthetics to a God which is supposedly as simple as can be conceived, were\\ndecisively rejected in Mackie\\'s \"The Miracle of Theism\".  In the revised\\nedition of \"The Existence of God\", Swinburne includes an Appendix in which he\\nmakes a somewhat incoherent attempt to rebut Mackie.\\n\\nJ. L. MACKIE\\n\\n\"The Miracle of Theism\", Oxford\\nThis (posthumous) volume contains a comprehensive review of the principal\\narguments for and against the existence of God.  It ranges from the classical\\nphilosophical positions of Descartes, Anselm, Berkeley, Hume et al, through\\nthe moral arguments of Newman, Kant and Sidgwick, to the recent restatements\\nof the classical theses by Plantinga and Swinburne.  It also addresses those\\npositions which push the concept of God beyond the realm of the rational,\\nsuch as those of Kierkegaard, Kung and Philips, as well as \"replacements for\\nGod\" such as Lelie\\'s axiarchism.  The book is a delight to read - less\\nformalistic and better written than Martin\\'s works, and refreshingly direct\\nwhen compared with the hand-waving of Swinburne.\\n\\nJAMES A. HAUGHT\\n\\n\"Holy Horrors: An Illustrated History of Religious Murder and Madness\",\\n Prometheus Books\\nLooks at religious persecution from ancient times to the present day -- and\\nnot only by Christians.\\nLibrary of Congress Catalog Card Number 89-64079. 1990.\\n\\nNORM R. ALLEN, JR.\\n\\n\"African American Humanism: an Anthology\"\\nSee the listing for African Americans for Humanism above.\\n\\nGORDON STEIN\\n\\n\"An Anthology of Atheism and Rationalism\", Prometheus Books\\nAn anthology covering a wide range of subjects, including \\'The Devil, Evil\\nand Morality\\' and \\'The History of Freethought\\'.  Comprehensive bibliography.\\n\\nEDMUND D. COHEN\\n\\n\"The Mind of The Bible-Believer\", Prometheus Books\\nA study of why people become Christian fundamentalists, and what effect it\\nhas on them.\\n\\n                                Net Resources\\n\\nThere\\'s a small mail-based archive server at mantis.co.uk which carries\\narchives of old alt.atheism.moderated articles and assorted other files.  For\\nmore information, send mail to archive-server@mantis.co.uk saying\\n\\n   help\\n   send atheism/index\\n\\nand it will mail back a reply.\\n\\n\\nmathew\\nÿ\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=open('documents/alt.atheism_49960.txt')\n",
    "text=data.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2zicR_aEXL2V",
    "outputId": "01a02cba-9811-4974-ad0a-f5e68d5a00d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 18828/18828 [37:18<00:00,  8.41it/s]\n"
     ]
    }
   ],
   "source": [
    "folder=os.listdir('documents')\n",
    "\n",
    "raw_text=[]\n",
    "email_data=[]\n",
    "subject_data=[]\n",
    "processed_text=[]\n",
    "file_name=[]\n",
    "class_details=[]\n",
    "\n",
    "for file in tqdm(folder):\n",
    "    class_name=file.split('_')[0]\n",
    "    doc_name=file.split('_')[1]\n",
    "    \n",
    "    data=open('documents/'+file)\n",
    "    input_text=data.read()\n",
    "    \n",
    "    email_file,subject,final_text=preprocess(input_text)\n",
    "    \n",
    "    raw_text.append(input_text)\n",
    "    email_data.append(email_file)\n",
    "    subject_data.append(subject)\n",
    "    processed_text.append(final_text)\n",
    "    file_name.append(doc_name)\n",
    "    class_details.append(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "66K7ebaQXL2W"
   },
   "outputs": [],
   "source": [
    "final_data=[raw_text,\n",
    "email_data,\n",
    "subject_data,\n",
    "processed_text,\n",
    "file_name,\n",
    "class_details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IEuCHToXXL2W",
    "outputId": "ad222094-1265-45a5-b90c-7d53a11b43c4"
   },
   "outputs": [],
   "source": [
    "with open('processed_data.pickle','wb') as fe_data_file:\n",
    "     pickle.dump(final_data, fe_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SoKXkyFOX5Jq"
   },
   "outputs": [],
   "source": [
    "location='/content/drive/MyDrive/CNN Assignment/processed_data.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kEQgDZSLXL2W"
   },
   "outputs": [],
   "source": [
    "with open(location,'rb') as fe_data_file:\n",
    "    final_data=pickle.load(fe_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MapwxuZAXL2X"
   },
   "outputs": [],
   "source": [
    "raw_text=final_data[0]\n",
    "email_data=final_data[1]\n",
    "subject_data=final_data[2]\n",
    "processed_text=final_data[3]\n",
    "file_name=final_data[4]\n",
    "class_details=final_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XY0FXebuXL2X"
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(list(zip(raw_text,email_data,subject_data,processed_text,file_name,class_details)),\n",
    "             columns=['text','email_id','subject','processed_text','file_name','class'])\n",
    "x=data['email_id']+data['subject']+ data['processed_text']\n",
    "y=data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yVzkZ0tRXL2X"
   },
   "outputs": [],
   "source": [
    "total_class=list(data['class'].unique())\n",
    "class_labels=dict()\n",
    "for i in range(len(total_class)):\n",
    "    class_labels[i]=total_class[i]\n",
    "    y[y==total_class[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMkLVJF0XL2Y",
    "outputId": "784181d3-abe1-49e7-de21-bdeae75d4631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  7., ...,  5., 18., 12.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceASjKizYv1U"
   },
   "source": [
    "### Code checking:\n",
    "\n",
    "<font color='red' size=4>\n",
    "After Writing preprocess function. call that functoin with the input text of 'alt.atheism_49960' doc and print the output of the preprocess function\n",
    "<br>\n",
    "This will help us to evaluate faster, based on the output we can suggest you if there are any changes.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x3og_iaYv1S"
   },
   "source": [
    "### After writing Preprocess function, call the function for each of the document(18828 docs) and then create a dataframe as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3ucJLtWYv1V"
   },
   "source": [
    "### Training The models to Classify: \n",
    "\n",
    "<pre>\n",
    "1. Combine \"preprocessed_text\", \"preprocessed_subject\", \"preprocessed_emails\" into one column. use that column to model. \n",
    "\n",
    "2. Now Split the data into Train and test. use 25% for test also do a stratify split. \n",
    "\n",
    "3. Analyze your text data and pad the sequnce if required. \n",
    "Sequnce length is not restricted, you can use anything of your choice. \n",
    "you need to give the reasoning\n",
    "\n",
    "4. Do Tokenizer i.e convert text into numbers. please be careful while doing it. \n",
    "if you are using tf.keras \"Tokenizer\" API, it removes the <b>\"_\"</b>, but we need that.\n",
    "\n",
    "5. code the model's ( Model-1, Model-2 ) as discussed below \n",
    "and try to optimize that models.  \n",
    "\n",
    "6. For every model use predefined Glove vectors. \n",
    "<b>Don't train any word vectors while Training the model.</b>\n",
    "\n",
    "7. Use \"categorical_crossentropy\" as Loss. \n",
    "\n",
    "8. Use <b>Accuracy and Micro Avgeraged F1 score</b> as your as Key metrics to evaluate your model. \n",
    "\n",
    "9.  Use Tensorboard to plot the loss and Metrics based on the epoches.\n",
    "\n",
    "10. Please save your best model weights in to <b>'best_model_L.h5' ( L = 1 or 2 )</b>. \n",
    "\n",
    "11. You are free to choose any Activation function, learning rate, optimizer.\n",
    "But have to use the same architecture which we are giving below.\n",
    "\n",
    "12. You can add some layer to our architecture but you <b>deletion</b> of layer is not acceptable.\n",
    "\n",
    "13. Try to use <b>Early Stopping</b> technique or any of the callback techniques that you did in the previous assignments.\n",
    "\n",
    "14. For Every model save your model to image ( Plot the model) with shapes \n",
    "and inlcude those images in the notebook markdown cell, \n",
    "upload those imgages to Classroom. You can use \"plot_model\" \n",
    "please refer <a href='https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model'>this</a> if you don't know how to plot the model with shapes. \n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCMt3JqIXL2Z",
    "outputId": "8d9a2539-fedd-483f-dd86-d721dac78601"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14121,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25, stratify=y)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNTEd64OWn1-"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RBqYiOgY60k"
   },
   "outputs": [],
   "source": [
    "'''with open('/content/drive/MyDrive/CNN Assignment/y_train.pkl','wb') as w:\n",
    "  pickle.dump(y_train,w)\n",
    "with open('/content/drive/MyDrive/CNN Assignment/y_test.pkl','wb') as w:\n",
    "  pickle.dump(y_test,w)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wy7iM_PyKfRf"
   },
   "outputs": [],
   "source": [
    "embedded_mat='/content/drive/MyDrive/CNN Assignment/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOnDn1r1KcDI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "trained_word_embedding=pd.read_csv(embedded_mat, sep=' ', header=None,lineterminator='\\n',quoting=csv.QUOTE_NONE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu7KXId-XL2Z"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/nlp-preparing-text-for-deep-learning-model-using-tensorflow2-461428138657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuO2ZAACXL2Z"
   },
   "outputs": [],
   "source": [
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token='UNK')\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uhi8OII-KrWG"
   },
   "outputs": [],
   "source": [
    "trained_embedded_words=trained_word_embedding[0]\n",
    "trained_word_embedding_vector=trained_word_embedding.drop(0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ew_7hxBPZ9L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAam5wwEK3VA",
    "outputId": "c6373c79-ed06-4ee9-d00c-82ad50630ac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.word_index.keys())[1] in trained_embedded_words.values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0pAXm-8Kv8f",
    "outputId": "5df0099b-5b61-4b4f-9f65-9840195c46dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64267it [06:48, 157.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words_list=[]\n",
    "for i,word in tqdm(enumerate(list(tokenizer.word_index.keys()))):\n",
    "  if word in trained_embedded_words.values.tolist():\n",
    "    words_list.append((word,i))\n",
    "    #print(word)\n",
    "print(len(words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT0_UcC7PUq5"
   },
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "embedding_matrix=np.zeros((len(words_list)+1,trained_word_embedding_vector.shape[1]))\n",
    "\n",
    "for i,words in enumerate(words_list):\n",
    "    word_dict[words[0].lower()]= i\n",
    "    embedding_matrix[i]=trained_word_embedding_vector.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0lETvGeUAE1"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index=word_dict\n",
    "tokenizer.word_index[tokenizer.oov_token] = max(word_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5qfCtlLQxH3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTWW3qcPRhIG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "023LQg-XXL2a"
   },
   "outputs": [],
   "source": [
    "x_train_tokenized=tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokenized=tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUkWzwBdXL2a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sgY7X5AXL2a",
    "outputId": "42a6282d-5d80-499e-ddc6-6ee41e7ea099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42686"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_values_in_list=[]\n",
    "for j in x_train_tokenized:\n",
    "    value=max(j)\n",
    "    max_values_in_list.append(value)\n",
    "    \n",
    "input_dim=max(max_values_in_list)\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yl2GkF0ZXL2b"
   },
   "outputs": [],
   "source": [
    "padded_x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train_tokenized,maxlen=len(tokenizer.word_index)+1 , padding='pre')\n",
    "padded_x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test_tokenized,maxlen=len(tokenizer.word_index)+1 , padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBwWswMWXL2b",
    "outputId": "fca191ad-e080-46b0-c122-38aedcc7f169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14121, 42688)\n",
      "(4707, 42688)\n"
     ]
    }
   ],
   "source": [
    "print(padded_x_train.shape)\n",
    "print(padded_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl0QAlKtB9ZK"
   },
   "outputs": [],
   "source": [
    "'''with open('/content/drive/MyDrive/CNN Assignment/padded_x_train.pkl','wb') as w:\n",
    "  pickle.dump(padded_x_train,w)\n",
    "\n",
    "with open('/content/drive/MyDrive/CNN Assignment/padded_x_test.pkl','wb') as w:\n",
    "  pickle.dump(padded_x_test,w)\n",
    "  \n",
    "with open('/content/drive/MyDrive/CNN Assignment/embedding_matrix.pkl','wb') as w:\n",
    "  pickle.dump(embedding_matrix,w)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PM3murdQB6Tw"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/CNN Assignment/padded_x_test.pkl','rb') as w:\n",
    "  padded_x_test=pickle.load(w)\n",
    "\n",
    "with open('/content/drive/MyDrive/CNN Assignment/padded_x_train.pkl','rb') as w:\n",
    "  padded_x_train=pickle.load(w)\n",
    "\n",
    "with open('/content/drive/MyDrive/CNN Assignment/embedding_matrix.pkl','rb') as w:\n",
    "  embedding_matrix=pickle.load(w)\n",
    "\n",
    "with open('/content/drive/MyDrive/CNN Assignment/y_test.pkl','rb') as w:\n",
    "  y_test=pickle.load(w)\n",
    "\n",
    "with open('/content/drive/MyDrive/CNN Assignment/y_train.pkl','rb') as w:\n",
    "  y_train=pickle.load(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWUR49m8T0eM",
    "outputId": "80e84815-139e-467e-db48-ca9f2f77ff3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43169, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0mwdtcvYv1X"
   },
   "source": [
    "### Model-1: Using 1D convolutions with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXPPsovJ3ePk"
   },
   "source": [
    "<pre>\n",
    "<b>Encoding of the Text </b> --> For a given text data create a Matrix with Embedding layer as shown Below. \n",
    "In the example we have considered d = 5, but in this assignment we will get d = dimension of Word vectors we are using.\n",
    " i.e if we have maximum of 350 words in a sentence and embedding of 300 dim word vector, \n",
    " we result in 350*300 dimensional matrix for each sentance as output after embedding layer\n",
    "<img src='https://i.imgur.com/kiVQuk1.png'>\n",
    "Ref: https://i.imgur.com/kiVQuk1.png\n",
    "\n",
    "<b>Reference:</b>\n",
    "<a href='https://stackoverflow.com/a/43399308/4084039'>https://stackoverflow.com/a/43399308/4084039</a>\n",
    "<a href='https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/'>https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/</a>\n",
    "\n",
    "<b><a href='https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work'>How EMBEDDING LAYER WORKS </a></b>\n",
    "\n",
    "</pre>\n",
    "\n",
    "### Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "udYkf-0_XL2c",
    "outputId": "d37b6ca4-d9db-406d-aac8-27f0389f3b72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"#Clearing Logs before running the Model\\nimport shutil\\nshutil.rmtree('logs/model_1')\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Clearing Logs before running the Model\n",
    "import shutil\n",
    "shutil.rmtree('logs/model_1')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRlIpJwMXL2d",
    "outputId": "3fc5ef26-c0c8-49ac-cd21-be1d32af77da",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 42688)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 42688, 100)   4268700     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 42688, 100)   4268700     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 42688, 100)   4268700     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 14229, 7)     2107        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 14229, 9)     2709        ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 14229, 11)    3311        ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 14229, 7)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 14229, 9)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 14229, 11)    0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14229, 27)    0           ['dropout[0][0]',                \n",
      "                                                                  'dropout_1[0][0]',              \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 7114, 27)     0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 2370, 5)      680         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 2370, 10)     1360        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 2370, 15)     2040        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 2370, 5)      0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 2370, 10)     0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 2370, 15)     0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2370, 30)     0           ['dropout_3[0][0]',              \n",
      "                                                                  'dropout_4[0][0]',              \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 1184, 30)    0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 394, 15)      1365        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 394, 15)      0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 5910)         0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 5910)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           118220      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,937,892\n",
      "Trainable params: 12,937,892\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "input_shape=(padded_x_train.shape[1])#input is just of this dimension\n",
    "inputs = tf.keras.Input(input_shape)\n",
    "\n",
    "conv_m=tf.keras.layers.Embedding(input_dim=(42686+1),output_dim=100,input_shape=(1,input_shape),embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=True)(inputs) #Input shape=(Batch_size, Size of input)\n",
    "conv_m=tf.keras.layers.Conv1D(7,3,strides=3,activation='elu',input_shape=conv_m.shape, kernel_initializer=tf.keras.initializers.HeUniform(1))(conv_m)\n",
    "conv_m=tf.keras.layers.Dropout(0.3)(conv_m)\n",
    "\n",
    "conv_n=tf.keras.layers.Embedding(input_dim=(42686+1),output_dim=100,input_shape=(1,input_shape))(inputs) #Input shape=(Batch_size, Size of input)\n",
    "conv_n=tf.keras.layers.Conv1D(9,3,strides=3,activation='elu',input_shape=conv_n.shape, kernel_initializer=tf.keras.initializers.HeUniform(3))(conv_n)\n",
    "conv_n=tf.keras.layers.Dropout(0.3)(conv_n)\n",
    "\n",
    "conv_o=tf.keras.layers.Embedding(input_dim=(42686+1),output_dim=100,input_shape=(1,input_shape))(inputs) #Input shape=(Batch_size, Size of input)\n",
    "conv_o=tf.keras.layers.Conv1D(11,3,strides=3,activation='elu',input_shape=conv_o.shape, kernel_initializer=tf.keras.initializers.HeUniform(5))(conv_o)\n",
    "conv_o=tf.keras.layers.Dropout(0.3)(conv_o)\n",
    "\n",
    "output_1= tf.keras.layers.concatenate([conv_m,conv_n,conv_o])\n",
    "\n",
    "output_1=tf.keras.layers.MaxPool1D(pool_size=3, strides=2)(output_1)\n",
    "\n",
    "conv_i=tf.keras.layers.Conv1D(5,5,strides=3,activation='elu',input_shape=output_1.shape, kernel_initializer=tf.keras.initializers.HeUniform(7))(output_1)\n",
    "conv_i=tf.keras.layers.Dropout(0.1)(conv_i)\n",
    "\n",
    "conv_j=tf.keras.layers.Conv1D(10,5,strides=3,activation='elu',input_shape=output_1.shape, kernel_initializer=tf.keras.initializers.HeUniform(9))(output_1)\n",
    "conv_j=tf.keras.layers.Dropout(0.1)(conv_j)\n",
    "\n",
    "conv_k=tf.keras.layers.Conv1D(15,5,strides=3,activation='elu',input_shape=output_1.shape, kernel_initializer=tf.keras.initializers.HeUniform(11))(output_1)\n",
    "conv_k=tf.keras.layers.Dropout(0.1)(conv_k)\n",
    "\n",
    "output_2= tf.keras.layers.concatenate([conv_i,conv_j,conv_k])\n",
    "\n",
    "output_2=tf.keras.layers.MaxPool1D(pool_size=3, strides=2)(output_2)\n",
    "output_2=tf.keras.layers.Conv1D(15,3,strides=3,activation='elu', kernel_initializer=tf.keras.initializers.HeUniform(150))(output_2)\n",
    "output_2=tf.keras.layers.Dropout(0.3)(output_2)\n",
    "output_2=tf.keras.layers.Flatten()(output_2)\n",
    "output_2=tf.keras.layers.Dropout(0.2)(output_2)\n",
    "output_2=tf.keras.layers.Dense(20, activation = 'softmax')(output_2)\n",
    "\n",
    "model=tf.keras.Model(inputs=inputs,outputs=output_2)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSMjz6JYXL2d"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        file_path=\"/content/drive/MyDrive/CNN Assignment/model_dash/weights_model_1\"+'_epoch_'+str(epoch)+'.hdf5' \n",
    "        if epoch>=2:\n",
    "          if self.history['val_accuracy'][-1]>self.history['val_accuracy'][-2]:\n",
    "            self.model.save(file_path)\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "jV6a63uNXL2d",
    "outputId": "34bf638e-4eab-4c35-af97-2a870ce0919c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 10/706 [..............................] - ETA: 1:19 - loss: 1.7583 - accuracy: 0.4500 - f1_score: 0.0468"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-57f1021ffc93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                       \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                       threshold=0.5)], )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_x_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[1;32m   1158\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=3,monitor='val_accuracy',min_delta=0.0001)\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/CNN Assignment/logs')\n",
    "'''lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=754*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)'''\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy',tfa.metrics.F1Score(num_classes=10, \n",
    "                      average='micro',\n",
    "                      threshold=0.5)], )\n",
    "model.fit(padded_x_train,y_train,epochs=100,callbacks=[early_stopping,tensorboard],validation_split=0.2, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFdTY988XL2e"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/CNN Assignment/models/model_dash.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuFTXqx9indr"
   },
   "outputs": [],
   "source": [
    "model_1=tf.keras.models.load_model('/content/drive/MyDrive/CNN Assignment/models/model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIiJyS5wXL2e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {'./logs'}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5apqJarXXL2e",
    "outputId": "4c4186f0-3f64-4950-bc2d-7e71f8fcec4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 7s 47ms/step - loss: 1.6366 - accuracy: 0.5963 - f1_score: 0.0721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.636583924293518, 0.596345841884613, 0.07212662696838379]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(padded_x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY_j2UD_XL2e"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='model_2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKynkl9xXL2e"
   },
   "source": [
    "<img src='https://i.imgur.com/fv1GvFJ.png'>\n",
    "ref: 'https://i.imgur.com/fv1GvFJ.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aHqPbMzXL2f"
   },
   "source": [
    "<pre>\n",
    "1. all are Conv1D layers with any number of filter and filter sizes, there is no restriction on this.\n",
    "\n",
    "2. use concatenate layer is to concatenate all the filters/channels. \n",
    "\n",
    "3. You can use any pool size and stride for maxpooling layer.\n",
    "\n",
    "4. Don't use more than 16 filters in one Conv layer becuase it will increase the no of params. \n",
    "( Only recommendation if you have less computing power )\n",
    "\n",
    "5. You can use any number of layers after the Flatten Layer.\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ceASjKizYv1U",
    "2x3og_iaYv1S",
    "c0mwdtcvYv1X"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
